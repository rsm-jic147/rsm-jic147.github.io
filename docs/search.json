[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n\n\n\n\nJiayin Chen\n\n\nMay 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nJiayin Chen\n\n\nApr 20, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, recipients were randomly assigned to different variations of the fundraising letter. Some were told their donation would be matched at a rate of 1:1, 2:1, or 3:1 by a “concerned fellow member,” up to a maximum matching amount ($25,000, $50,000, or $100,000). Others were part of the control group, which received no mention of a match. Suggested donation amounts were also varied to test the impact of different “ask” levels. The researchers found that simply including a match offer increased both donation rates and average amounts raised—but larger match ratios (2:1 or 3:1) did not generate significantly higher donations than the 1:1 match.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, recipients were randomly assigned to different variations of the fundraising letter. Some were told their donation would be matched at a rate of 1:1, 2:1, or 3:1 by a “concerned fellow member,” up to a maximum matching amount ($25,000, $50,000, or $100,000). Others were part of the control group, which received no mention of a match. Suggested donation amounts were also varied to test the impact of different “ask” levels. The researchers found that simply including a match offer increased both donation rates and average amounts raised—but larger match ratios (2:1 or 3:1) did not generate significantly higher donations than the 1:1 match.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset consists of 50,083 observations and 51 variables, drawn from a large-scale field experiment conducted by Karlan and List (2007) to study charitable giving behavior. Each observation represents a past donor who was randomly assigned to receive one of several fundraising letter treatments. These treatments varied in terms of match ratio, match threshold, and suggested donation amount.\nThe dataset includes:\nTreatment assignment variables, indicating whether individuals were in the control group or received a match offer, and if so, the match ratio (e.g., 1:1, 2:1, 3:1) and threshold size ($25,000, $50,000, $100,000, or unstated).\nSuggested donation amounts based on individuals’ previous giving history, which were randomly varied in the letters.\nDonation outcomes, including whether the individual gave and how much they gave.\nDemographic and donation history, such as gender, relationship status, number of years since the first donation, and prior donation frequency.\nGeographic and political context, with data on the political leaning of the recipient’s state and county (e.g., red/blue states), as well as census-linked socioeconomic indicators like median income, education, and urbanicity at the zip code level.\n\nimport pandas as pd\n\n# Load the Stata file\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Show the first few rows\nprint(df.head())\n\n   treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\n\n# Variables to test for balance\nvariables = ['mrm2', 'freq', 'years', 'female', 'ave_hh_sz', 'median_hhincome']\n\nprint(\"=== Balance Check: Treatment vs Control ===\\n\")\n\nfor var in variables:\n    # Drop missing for current variable\n    df_clean = df[[var, 'treatment']].dropna()\n\n    # Split by group\n    treat = df_clean[df_clean['treatment'] == 1][var]\n    control = df_clean[df_clean['treatment'] == 0][var]\n\n    # Sample sizes\n    n1 = len(treat)\n    n0 = len(control)\n\n    # Means\n    mean1 = treat.mean()\n    mean0 = control.mean()\n\n    # Variances\n    var1 = treat.var(ddof=1)\n    var0 = control.var(ddof=1)\n\n    # T-statistic\n    se = np.sqrt(var1 / n1 + var0 / n0)\n    t_stat = (mean1 - mean0) / se\n    df_deg = min(n1 - 1, n0 - 1)\n    p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df_deg))\n\n    # Print results\n    print(f\"Variable: {var}\")\n    print(f\"  Control Mean:   {mean0:.3f}\")\n    print(f\"  Treatment Mean: {mean1:.3f}\")\n    print(f\"  t-statistic:    {t_stat:.3f}\")\n    print(f\"  p-value:        {p_val:.4f}\")\n    print(f\"  Statistically significant? {'YES' if p_val &lt; 0.05 else 'NO'}\\n\")\n\n=== Balance Check: Treatment vs Control ===\n\nVariable: mrm2\n  Control Mean:   12.998\n  Treatment Mean: 13.012\n  t-statistic:    0.120\n  p-value:        0.9049\n  Statistically significant? NO\n\nVariable: freq\n  Control Mean:   8.047\n  Treatment Mean: 8.035\n  t-statistic:    -0.111\n  p-value:        0.9117\n  Statistically significant? NO\n\nVariable: years\n  Control Mean:   6.136\n  Treatment Mean: 6.078\n  t-statistic:    -1.091\n  p-value:        0.2753\n  Statistically significant? NO\n\nVariable: female\n  Control Mean:   0.283\n  Treatment Mean: 0.275\n  t-statistic:    -1.754\n  p-value:        0.0795\n  Statistically significant? NO\n\nVariable: ave_hh_sz\n  Control Mean:   2.427\n  Treatment Mean: 2.430\n  t-statistic:    0.823\n  p-value:        0.4103\n  Statistically significant? NO\n\nVariable: median_hhincome\n  Control Mean:   54921.094\n  Treatment Mean: 54763.169\n  t-statistic:    -0.743\n  p-value:        0.4573\n  Statistically significant? NO\n\n\n\nEach of these variables shows no statistically significant difference at the 95% confidence level, including female, which has the lowest p-value (0.0795) but still does not cross the conventional threshold for significance.\nThese findings replicate the results shown in Table 1 of Karlan & List (2007). The table in the original paper is designed to demonstrate that randomization worked: treatment and control groups are statistically equivalent in expectation across observable characteristics.\nBy showing balance across variables like mrm2, freq, female, and neighborhood demographics (median_hhincome, ave_hh_sz), we can be confident that:\n\nThere is no selection bias\nAny differences in donation outcomes observed later can be causally attributed to the treatment"
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n## bar plot\n\nimport matplotlib.pyplot as plt\n\n# Calculate proportions\ngave_by_group = df[['gave', 'treatment']].dropna().groupby('treatment')['gave'].mean()\nlabels = ['Control', 'Treatment']\nproportions = [gave_by_group[0], gave_by_group[1]]\n\n# Create bar plot\nplt.figure(figsize=(6, 4))\nbars = plt.bar(labels, proportions)\nplt.ylabel('Proportion Who Donated')\nplt.title('Donation Rates by Group')\nplt.ylim(0, max(proportions) + 0.05)\n\n# Add value labels\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.3f}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# t-tes\n\n# T-test and regression for the binary outcome 'gave'\n\n# Drop missing values for 'gave' and 'treatment'\ngave_data = df[['gave', 'treatment']].dropna()\n\n# Split into treatment and control\ngave_treatment = gave_data[gave_data['treatment'] == 1]['gave']\ngave_control = gave_data[gave_data['treatment'] == 0]['gave']\n\n# Manual t-test\nn1 = len(gave_treatment)\nn0 = len(gave_control)\nmean1 = gave_treatment.mean()\nmean0 = gave_control.mean()\nvar1 = gave_treatment.var(ddof=1)\nvar0 = gave_control.var(ddof=1)\nse = np.sqrt(var1/n1 + var0/n0)\nt_stat = (mean1 - mean0) / se\ndf_deg = min(n1 - 1, n0 - 1)\np_val = 2 * (1 - stats.t.cdf(abs(t_stat), df_deg))\n\n# Bivariate linear regression\nX = sm.add_constant(gave_data['treatment'])\ny = gave_data['gave']\ngave_model = sm.OLS(y, X).fit()\n\n{\n    \"t_test\": {\n        \"mean_treatment\": mean1,\n        \"mean_control\": mean0,\n        \"t_statistic\": t_stat,\n        \"p_value\": p_val\n    },\n    \"regression_coef\": gave_model.params['treatment'],\n    \"regression_pval\": gave_model.pvalues['treatment'],\n    \"regression_summary\": gave_model.summary().as_text()\n}\n# --- Print nicely formatted results ---\n\nprint(\"=== T-Test Results ===\")\nprint(f\"Treatment group donation rate: {mean1:.4f}\")\nprint(f\"Control group donation rate:   {mean0:.4f}\")\nprint(f\"T-statistic: {t_stat:.3f}\")\nprint(f\"P-value:     {p_val:.4f}\")\n\nprint(\"\\n=== Regression Results ===\")\nprint(f\"Estimated treatment effect (coef): {gave_model.params['treatment']:.4f}\")\nprint(f\"Standard error:                   {gave_model.bse['treatment']:.4f}\")\nprint(f\"T-statistic:                      {gave_model.tvalues['treatment']:.3f}\")\nprint(f\"P-value:                          {gave_model.pvalues['treatment']:.4f}\")\n\n=== T-Test Results ===\nTreatment group donation rate: 0.0220\nControl group donation rate:   0.0179\nT-statistic: 3.209\nP-value:     0.0013\n\n=== Regression Results ===\nEstimated treatment effect (coef): 0.0042\nStandard error:                   0.0013\nT-statistic:                      3.101\nP-value:                          0.0019\n\n\nThese results align with Table 2A, Panel A of Karlan & List (2007), which also shows a higher donation rate in the treatment group.\nWhile the increase in donation probability is small in absolute terms, it is statistically meaningful. This tells us something important about human behavior: small cues or framing changes (like mentioning a matching grant) can make people more likely to give.\nIn particular, this experiment shows that:\n\nPsychological nudges, such as the presence of a matching donor, can shift decisions.\nEven minimal changes in language or perceived impact can meaningfully influence behavior.\nPeople respond to social signals of generosity, even if the financial incentive is unchanged.\n\nFrom a behavioral perspective, this suggests: - People are more likely to give when they believe their donation will be matched — it increases the perceived impact of their gift. - This change in behavior is not driven by financial incentives alone (since the match isn’t actually paid to them) but by psychological framing and the feeling that their donation will be “worth more.”\nIn the context of the Karlan & List study, this result supports the idea that small nudges, such as the presence of a matching grant, can meaningfully shift behavior — a key finding for fundraisers and behavioral economists alike.\n\n## replicate of table 3\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Clean the data\ndf_clean = df[['gave', 'treatment']].dropna()\n\n# Probit regression\nprobit_model = smf.probit('gave ~ treatment', data=df_clean).fit(disp=0)\n\n# Get marginal effects\nmfx = probit_model.get_margeff(method='dydx').summary_frame()\n\n# Display results\nprint(\"\\n=== Probit Regression (Marginal Effects) ===\")\nprint(mfx)\n\n\n=== Probit Regression (Marginal Effects) ===\n              dy/dx  Std. Err.         z  Pr(&gt;|z|)  Conf. Int. Low  \\\ntreatment  0.004313   0.001389  3.104419  0.001907         0.00159   \n\n           Cont. Int. Hi.  \ntreatment        0.007036  \n\n\nTo replicate Table 3, Column 1 from Karlan & List (2007), I ran a probit regression where the outcome variable was whether an individual made any charitable donation (gave) and the explanatory variable was assignment to the treatment or control group.\nI then calculated marginal effects to make the results directly comparable to those reported in the paper.\nThe estimated marginal effect of the treatment was 0.0043, with a standard error of 0.0014 and a p-value of 0.0019. This means that being assigned to the treatment group increased the probability of donating by about 0.43 percentage points. The result is statistically significant at the 1% level, matching the original paper’s finding of a 0.004* effect.\nThis confirms the authors’ key result: framing the ask as part of a matching donation offer has a measurable and statistically significant impact on giving behavior. Even though the increase in donation probability is relatively small, it provides compelling evidence that simple nudges — like mentioning a match — can influence real-world decisions in meaningful ways.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# t-test\nprint(df['ratio'].unique())\n\n# Filter to treatment group only with valid match ratio and donation status\ndf_filtered = df[(df['treatment'] == 1) & (df['gave'].notnull()) & (df['ratio'].isin([1, 2, 3]))]\n\n# Subset donation status by match ratio\ngave_1to1 = df_filtered[df_filtered['ratio'] == 1]['gave']\ngave_2to1 = df_filtered[df_filtered['ratio'] == 2]['gave']\ngave_3to1 = df_filtered[df_filtered['ratio'] == 3]['gave']\n\n# Compute mean response rates\nmeans = {\n    \"1:1\": gave_1to1.mean(),\n    \"2:1\": gave_2to1.mean(),\n    \"3:1\": gave_3to1.mean()\n}\n\n# Conduct pairwise t-tests\nt_2v1 = stats.ttest_ind(gave_2to1, gave_1to1, equal_var=False)\nt_3v1 = stats.ttest_ind(gave_3to1, gave_1to1, equal_var=False)\nt_3v2 = stats.ttest_ind(gave_3to1, gave_2to1, equal_var=False)\n\n# Print results\nprint(\"=== Mean Donation Rates ===\")\nfor k, v in means.items():\n    print(f\"{k} match: {v:.4%}\")\n\nprint(\"\\n=== T-Test Results ===\")\nprint(f\"2:1 vs 1:1: t = {t_2v1.statistic:.3f}, p = {t_2v1.pvalue:.4f}\")\nprint(f\"3:1 vs 1:1: t = {t_3v1.statistic:.3f}, p = {t_3v1.pvalue:.4f}\")\nprint(f\"3:1 vs 2:1: t = {t_3v2.statistic:.3f}, p = {t_3v2.pvalue:.4f}\")\n\n# Plot response rates by match ratio\nplt.figure(figsize=(6, 4))\nplt.bar(means.keys(), means.values(), color=['#4daf4a', '#377eb8', '#984ea3'])\nplt.ylabel('Proportion Who Donated')\nplt.title('Donation Rates by Match Ratio')\n\n# Add value labels\nfor label, val in means.items():\n    plt.text(label, val + 0.0005, f'{val:.3%}', ha='center')\n\nplt.ylim(0, max(means.values()) + 0.01)\nplt.tight_layout()\nplt.show()\n\n['Control', 1, 2, 3]\nCategories (4, object): ['Control' &lt; 1 &lt; 2 &lt; 3]\n=== Mean Donation Rates ===\n1:1 match: 2.0749%\n2:1 match: 2.2633%\n3:1 match: 2.2733%\n\n=== T-Test Results ===\n2:1 vs 1:1: t = 0.965, p = 0.3345\n3:1 vs 1:1: t = 1.015, p = 0.3101\n3:1 vs 2:1: t = 0.050, p = 0.9600\n\n\n\n\n\n\n\n\n\nThe results fully support the authors’ interpretation on page 8.\n“While the match treatments relative to a control group increase the probability of donating, larger match ratios—$3:$1 and $2:$1—relative to a smaller match ratio ($1:$1) have no additional impact.”\nEven though the donation rates numerically increase with larger match ratios, the t-tests show these increases are not statistically significant.\n\n# Filter dataset: keep all rows with non-null 'gave' and any valid 'ratio'\ndf_ratio_all = df[df['gave'].notnull() & df['ratio'].isin(['Control', 1, 2, 3])].copy()\n\n# Ensure 'ratio' is treated as a categorical variable\ndf_ratio_all['ratio'] = df_ratio_all['ratio'].astype('category')\n\n# Run regression with ratio as a categorical predictor\nmodel = smf.ols('gave ~ C(ratio)', data=df_ratio_all).fit()\n\n# Display results\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                  0.000\nMethod:                 Least Squares   F-statistic:                     3.665\nDate:                Tue, 06 May 2025   Prob (F-statistic):             0.0118\nTime:                        20:41:20   Log-Likelihood:                 26630.\nNo. Observations:               50083   AIC:                        -5.325e+04\nDf Residuals:                   50079   BIC:                        -5.322e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0179      0.001     16.225      0.000       0.016       0.020\nC(ratio)[T.1]     0.0029      0.002      1.661      0.097      -0.001       0.006\nC(ratio)[T.2]     0.0048      0.002      2.744      0.006       0.001       0.008\nC(ratio)[T.3]     0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\nOmnibus:                    59812.754   Durbin-Watson:                   2.005\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          4316693.217\nSkew:                           6.740   Prob(JB):                         0.00\nKurtosis:                      46.438   Cond. No.                         4.26\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nTo assess how different match ratios affect the likelihood of donating, I ran a linear regression of gave (a binary indicator for whether a person donated) on a categorical variable representing the match ratio. This includes the control group (no match), as well as the 1:1, 2:1, and 3:1 match conditions.\nThe regression treats the control group as the baseline category. The coefficients on the 1:1, 2:1, and 3:1 match indicators reflect the difference in donation rates compared to the control group.\n\nThe intercept represents the average donation rate in the control group (i.e., the probability of giving when no match is offered).\nThe coefficients for 1:1, 2:1, and 3:1 indicate how much more likely someone is to donate under each match condition compared to control.\n\nIf these coefficients are statistically significant, we can conclude that the corresponding match offer increased the likelihood of giving relative to the control group.\n\n# data vs. regression\n\n# Step 1: Direct differences from the data\ngave_1 = df_ratio_all[df_ratio_all['ratio'] == 1]['gave']\ngave_2 = df_ratio_all[df_ratio_all['ratio'] == 2]['gave']\ngave_3 = df_ratio_all[df_ratio_all['ratio'] == 3]['gave']\n\ndirect_diff_21_vs_11 = gave_2.mean() - gave_1.mean()\ndirect_diff_31_vs_21 = gave_3.mean() - gave_2.mean()\n\n# Step 2: Use regression coefficients (relative to Control group)\ncoef_1 = model.params.get('C(ratio)[T.1]', 0)\ncoef_2 = model.params.get('C(ratio)[T.2]', 0)\ncoef_3 = model.params.get('C(ratio)[T.3]', 0)\n\nreg_diff_21_vs_11 = coef_2 - coef_1\nreg_diff_31_vs_21 = coef_3 - coef_2\n\n# Print results\nprint(\"=== Response Rate Differences ===\")\nprint(f\"2:1 vs 1:1 (Direct from data):       {direct_diff_21_vs_11:.4%}\")\nprint(f\"3:1 vs 2:1 (Direct from data):       {direct_diff_31_vs_21:.4%}\")\nprint(f\"2:1 vs 1:1 (From regression coef):   {reg_diff_21_vs_11:.4%}\")\nprint(f\"3:1 vs 2:1 (From regression coef):   {reg_diff_31_vs_21:.4%}\")\n\n=== Response Rate Differences ===\n2:1 vs 1:1 (Direct from data):       0.1884%\n3:1 vs 2:1 (Direct from data):       0.0100%\n2:1 vs 1:1 (From regression coef):   0.1884%\n3:1 vs 2:1 (From regression coef):   0.0100%\n\n\nThese values are nearly identical by both methods, confirming the robustness of the regression results. The 2:1 match rate yields a very small increase in response over 1:1, while the jump from 2:1 to 3:1 is virtually nonexistent. This reinforces the conclusion that larger match ratios beyond 1:1 do not meaningfully increase the likelihood of donation.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n# t-test  vs bivariate\n# Filter for treatment and control groups with valid donation amounts\ndf_amount = df[df['amount'].notnull() & df['treatment'].isin([0, 1])]\n\n# Perform a t-test on donation amounts between treatment and control\namount_treatment = df_amount[df_amount['treatment'] == 1]['amount']\namount_control = df_amount[df_amount['treatment'] == 0]['amount']\nt_stat, p_val = stats.ttest_ind(amount_treatment, amount_control, equal_var=False)\n\n# Run a bivariate linear regression of amount on treatment\namount_model = smf.ols('amount ~ treatment', data=df_amount).fit()\namount_summary = amount_model.summary2().as_text()\n\n# Extract regression results\ncoef = amount_model.params['treatment']\npval = amount_model.pvalues['treatment']\nconf_int = amount_model.conf_int().loc['treatment'].tolist()\n\n{\n    \"t_test\": {\n        \"t_statistic\": t_stat,\n        \"p_value\": p_val\n    },\n    \"regression\": {\n        \"coef\": coef,\n        \"p_value\": pval,\n        \"95% CI\": conf_int\n    },\n    \"regression_summary\": amount_summary\n}\n\nprint(\"=== Effect of Treatment on Donation Amount ===\")\n\n# T-test results\nprint(\"\\nT-test comparing treatment vs. control:\")\nprint(f\"t-statistic: {t_stat:.3f}\")\nprint(f\"p-value:     {p_val:.4f}\")\n\n# Regression results\nprint(\"\\nRegression of amount on treatment:\")\nprint(f\"Treatment coefficient: {coef:.3f}\")\nprint(f\"p-value:              {pval:.4f}\")\nprint(f\"95% Confidence Interval: [{conf_int[0]:.3f}, {conf_int[1]:.3f}]\")\n\n=== Effect of Treatment on Donation Amount ===\n\nT-test comparing treatment vs. control:\nt-statistic: 1.918\np-value:     0.0551\n\nRegression of amount on treatment:\nTreatment coefficient: 0.154\np-value:              0.0628\n95% Confidence Interval: [-0.008, 0.315]\n\n\nThis analysis helps us understand whether the treatment affects not just the likelihood of donating, but also how much people give. The results from both the t-test and regression show that people in the treatment group donate about $0.15 more on average than those in the control group. However, this difference is only marginally statistically significant (p ≈ 0.06), and the confidence interval includes zero.\nFrom this, we learn that while the treatment (a matching offer) strongly influences whether people donate, it has a much weaker and less certain effect on the amount given. This suggests that the psychological nudge of a match offer may primarily work by encouraging participation — getting more people to donate — rather than motivating donors to give significantly more money.\n\n## limited to donate\n\n# Filter to include only donors (gave = 1) with valid amount and treatment status\ndf_positive_donors = df[(df['gave'] == 1) & (df['amount'].notnull()) & (df['treatment'].isin([0, 1]))]\n\n# T-test: compare average donation amount between treatment and control groups among donors only\namount_treatment = df_positive_donors[df_positive_donors['treatment'] == 1]['amount']\namount_control = df_positive_donors[df_positive_donors['treatment'] == 0]['amount']\nt_stat, p_val = stats.ttest_ind(amount_treatment, amount_control, equal_var=False)\n\n# Regression: amount ~ treatment among donors\ndonor_model = smf.ols('amount ~ treatment', data=df_positive_donors).fit()\ncoef = donor_model.params['treatment']\npval = donor_model.pvalues['treatment']\nconf_int = donor_model.conf_int().loc['treatment'].tolist()\nsummary_text = donor_model.summary2().as_text()\n\n{\n    \"t_test\": {\n        \"t_statistic\": t_stat,\n        \"p_value\": p_val\n    },\n    \"regression\": {\n        \"coef\": coef,\n        \"p_value\": pval,\n        \"95% CI\": conf_int\n    },\n    \"regression_summary\": summary_text\n}\nprint(\"=== Effect of Treatment on Donation Amount (Among Donors Only) ===\")\n\n# T-test results\nprint(\"\\nT-test:\")\nprint(f\"t-statistic: {t_stat:.3f}\")\nprint(f\"p-value:     {p_val:.4f}\")\n\n# Regression results\nprint(\"\\nRegression:\")\nprint(f\"Treatment coefficient:     {coef:.3f}\")\nprint(f\"p-value:                   {pval:.4f}\")\nprint(f\"95% Confidence Interval:   [{conf_int[0]:.3f}, {conf_int[1]:.3f}]\")\n\n=== Effect of Treatment on Donation Amount (Among Donors Only) ===\n\nT-test:\nt-statistic: -0.585\np-value:     0.5590\n\nRegression:\nTreatment coefficient:     -1.668\np-value:                   0.5615\n95% Confidence Interval:   [-7.305, 3.968]\n\n\nInterpretation of Regression Coefficient (Among Donors Only)\nThe regression coefficient on treatment is -1.668, meaning that among those who donated, individuals in the treatment group gave about $1.67 less on average than those in the control group. However, this difference is not statistically significant (p = 0.561), and the confidence interval ([-7.31, 3.97]) includes zero, indicating high uncertainty in the estimate.\nThis tells us that conditional on donating, the treatment had no clear effect on the amount given.\nDoes the Coefficient Have a Causal Interpretation?\nNo — this coefficient does not have a causal interpretation. By restricting the analysis to donors only, we introduce selection bias, because treatment status may influence the likelihood of appearing in this subset (i.e., whether someone donates). Since the treatment affects who is included in the analysis, the estimated difference in donation amounts is not a valid causal estimate.\nInstead, this result is descriptive: it tells us that among donors, there is no significant difference in how much people gave depending on whether they were in the treatment group or not.\n\n## make 2 plot\n\n# Filter for donors with non-zero donation amounts and treatment assignment\ndf_donors = df[(df['gave'] == 1) & (df['amount'] &gt; 0) & (df['treatment'].isin([0, 1]))]\n\n# Split into control and treatment groups\ncontrol_donors = df_donors[df_donors['treatment'] == 0]['amount']\ntreatment_donors = df_donors[df_donors['treatment'] == 1]['amount']\n\n# Compute means\nmean_control = control_donors.mean()\nmean_treatment = treatment_donors.mean()\n\n# Plot histograms side by side\nfig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n\n# Control group histogram\naxes[0].hist(control_donors, bins=30, color='lightgray', edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='--', label=f'Mean = ${mean_control:.2f}')\naxes[0].set_title('Control Group: Donation Amounts')\naxes[0].set_xlabel('Donation Amount ($)')\naxes[0].set_ylabel('Number of Donors')\naxes[0].legend()\n\n# Treatment group histogram\naxes[1].hist(treatment_donors, bins=30, color='lightblue', edgecolor='black')\naxes[1].axvline(mean_treatment, color='red', linestyle='--', label=f'Mean = ${mean_treatment:.2f}')\naxes[1].set_title('Treatment Group: Donation Amounts')\naxes[1].set_xlabel('Donation Amount ($)')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese plots visually confirm what your regression showed earlier: Although the treatment group received matching offers, they donated slightly less on average than the control group, conditional on donating. However, the distributions are quite similar overall, and the difference in means is not statistically significant.\nThis supports the idea that:\n“Matching offers increase the likelihood of giving, but do not meaningfully affect how much people give, once they’ve already decided to donate.”"
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nn = 500  # sample size per group\nreps = 10000  # number of simulations\n\n# Simulate samples\ndiffs = []\nfor _ in range(reps):\n    control_sample = np.random.binomial(1, p_control, n)\n    treatment_sample = np.random.binomial(1, p_treatment, n)\n    diff = treatment_sample.mean() - control_sample.mean()\n    diffs.append(diff)\n\n# Convert to array\ndiffs = np.array(diffs)\n\n# Plot histogram of differences\nplt.figure(figsize=(8, 4))\nplt.hist(diffs, bins=50, color='skyblue', edgecolor='black', density=True)\nplt.axvline(np.mean(diffs), color='red', linestyle='--', label=f'Mean Diff = {np.mean(diffs):.4f}')\nplt.title('Sampling Distribution of Mean Differences (p=0.022 vs p=0.018)')\nplt.xlabel('Treatment Mean - Control Mean')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTo illustrate key concepts in statistical inference, I conducted a simulation based on the charitable giving experiment.\nSimulation Setup We assume two groups:\nControl group: probability of donating is p=0.018\nTreatment group (offered any size of match): probability of donating is p=0.022\nIn each simulation, I generated 500 observations from each group using a Bernoulli distribution (i.e., a 0 or 1 outcome representing whether someone donates). I repeated this process 10,000 times and recorded the difference in sample means between the treatment and control groups in each replication.\nResults The histogram of the simulated mean differences shows a clear bell-shaped distribution centered near 0.004, which is the true difference in population probabilities.\nInterpretation The Law of Large Numbers is reflected in how the average of the simulated differences converges toward the true difference ( 0.022−0.018=0.004) as we increase the number of replications.\nThe Central Limit Theorem explains why the distribution of these sample differences is approximately normal, even though the underlying data are binary (non-normal). This justifies using t-tests and confidence intervals in practice.\n\nLaw of Large Numbers\n\n# Simulation parameters\np_control = 0.018\np_treatment = 0.022\nn_control = 100000\nn_treatment = 10000\nsimulations = 10000\n\n# Simulate draws\ncontrol_sample = np.random.binomial(1, p_control, n_control)\ntreatment_sample = np.random.binomial(1, p_treatment, n_treatment)\n\n# Draw 10,000 random pairs and calculate differences\ndiffs = []\nfor _ in range(simulations):\n    control_draw = np.random.choice(control_sample)\n    treatment_draw = np.random.choice(treatment_sample)\n    diffs.append(treatment_draw - control_draw)\n\n# Convert to array and compute cumulative average\ndiffs = np.array(diffs)\ncumulative_avg = np.cumsum(diffs) / np.arange(1, simulations + 1)\n\n# Plot cumulative average with reference line at true mean difference (0.004)\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average', color='blue')\nplt.axhline(0.004, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title('Cumulative Average of Simulated Differences')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average Difference')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTo demonstrate the Law of Large Numbers (LLN) and reinforce the intuition behind the t-statistic, I simulated 100,000 binary outcomes from a control distribution with a probability of donation p=0.018, and 10,000 binary outcomes from a treatment distribution with p=0.022. These distributions represent the underlying probabilities of donation in each group from the original charitable giving experiment.\nFrom these two populations, I randomly drew 10,000 treatment–control pairs and computed the difference in outcomes for each pair. The plot above shows the cumulative average of these 10,000 differences, alongside the true difference in means (0.004) indicated by a red dashed line.\nWhat the Plot Shows\n\nThe cumulative average starts out noisy due to randomness in small samples.\nAs more differences are accumulated, the average stabilizes and converges toward the true population difference of 0.004.\nThis convergence visually confirms the Law of Large Numbers: with enough samples, the sample average approaches the expected value.\nIt also reinforces the foundation of the Central Limit Theorem — although each individual outcome is binary (not normally distributed), the sampling distribution of the mean becomes approximately normal.\n\nThe simulation shows that the cumulative average approaches the true difference in means between treatment and control groups. This not only demonstrates core statistical principles like the LLN and CLT, but also helps explain why test statistics like the t-statistic work reliably in large samples — even when the underlying data is not continuous or normally distributed.\n\n\nCentral Limit Theorem\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nreps = 1000\n\n# Set up plots\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\n# Loop over sample sizes and simulate mean differences\nfor i, n in enumerate(sample_sizes):\n    mean_diffs = []\n    for _ in range(reps):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        mean_diffs.append(diff)\n\n    # Plot histogram of mean differences\n    ax = axes[i]\n    ax.hist(mean_diffs, bins=30, color='skyblue', edgecolor='black', density=True)\n    ax.axvline(x=0, color='red', linestyle='--', label='Zero')\n    ax.axvline(x=np.mean(mean_diffs), color='blue', linestyle='-', label='Mean')\n    ax.set_title(f\"Sample Size: {n}\")\n    ax.set_xlabel(\"Mean Difference (Treatment - Control)\")\n    ax.set_ylabel(\"Density\")\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation\n\nAt small sample sizes (n = 50): The distribution is wide and somewhat irregular. Zero lies close to the center, meaning we often can’t distinguish treatment from control.\nAs the sample size increases: The distributions become tighter, more symmetrical, and more normal-shaped — a direct illustration of the CLT.\nBy n = 500 or 1000: The distribution is clearly centered around the true mean difference (~0.004), and zero shifts toward the tail, suggesting we now have enough data to statistically distinguish treatment from control in many simulations.\n\nThese plots demonstrate the Central Limit Theorem in action: as the sample size grows, the sampling distribution of the mean difference becomes more normal and more concentrated around the true value. The probability of observing an effect near zero — assuming the true difference is 0.004 — shrinks with larger sample sizes, improving our ability to detect a real effect."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/project2/hw2_questions.html",
    "href": "blog/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nTo start the analysis, we began by loading the blueprinty.csv dataset into our environment using pandas: The blueprinty dataset contains information on a sample of firms and includes both outcome and explanatory variables relevant for modeling innovation performance — particularly patent activity. The main variables in the dataset include:\n\npatents: Number of patents awarded to each firm over a 5-year period (count outcome).\nage: Firm age in years.\nregion: Categorical variable representing the firm’s geographical location.\niscustomer: Binary indicator (1 if the firm is a Blueprinty customer, 0 otherwise).\n\n\nimport pandas as pd\n\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\ndisplay(blueprinty.head())\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\n\nTo investigate whether Blueprinty customers tend to have more patents, we compared the distribution of patents for customers vs. non-customers both visually and numerically. To begin, we grouped the dataset by customer status (iscustomer) and calculated the average number of patents. The results show that Blueprinty customers average 4.13 patents, compared to 3.47 patents for non-customers. This suggests a potential positive association between software usage and innovation outcomes.\nTo visualize this relationship, we created a histogram of patent counts stratified by customer status. Both groups display a right-skewed distribution, typical of count data like patents. However, the histogram shows that Blueprinty customers are more concentrated at higher patent counts, with their distribution clearly shifted to the right relative to non-customers. This provides initial descriptive support for the hypothesis that Blueprinty software may enhance innovation performance.\nThis preliminary finding suggests a potential positive association between customer status and patent count, which will be investigated further using Poisson regression.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", binwidth=1, multiple=\"dodge\")\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n\nmeans_by_status = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean().reset_index()\ndisplay(means_by_status)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\npatents\n\n\n\n\n0\n0\n3.473013\n\n\n1\n1\n4.133056\n\n\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nTo assess whether Blueprinty customers differ systematically from non-customers, we explored variation in geographic region by customer status. This is an important step in evaluating potential confounding variables, which could bias the observed relationship between software usage and patent outcomes.\nWe visualized the regional distribution of firms across customer categories using a bar chart. The results reveal a clear pattern: the Northeast region accounts for a disproportionately high number of Blueprinty customers, while other regions — including the Midwest, Northwest, South, and Southwest — are more heavily populated by non-customers. This suggests that customer status is not randomly assigned with respect to geography.\n\n# Compare average age by customer status\nage = blueprinty.groupby(\"iscustomer\")[\"age\"]\n\n# Compare region distribution by customer status using counts\nregion_counts = pd.crosstab(blueprinty[\"region\"], blueprinty[\"iscustomer\"])\nregion_counts.columns = [\"Non-Customer\", \"Customer\"]\n\nimport matplotlib.pyplot as plt\n\n# Bar plot of regional distributions (counts)\nregion_counts.plot(kind=\"bar\", figsize=(10, 5))\nplt.title(\"Regional Distribution by Customer Status\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Region\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe implication is that region may influence both the likelihood of adopting Blueprinty software and the number of patents awarded. For instance, firms in the Northeast may have better access to innovation infrastructure, denser professional networks, or different industry concentrations — all of which could affect patenting behavior independently of software usage.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nWe model the number of patents \\(Y_i\\) for each firm \\(i\\) as Poisson-distributed:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function of the Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming independent observations for \\(i = 1, 2, \\dots, n\\), the likelihood function for the full sample is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the natural logarithm, the log-likelihood function becomes:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\nThis log-likelihood will be the basis for estimating the parameter \\(\\lambda\\) using Maximum Likelihood Estimation.\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    \"\"\"\n    Compute the log-likelihood of Poisson-distributed data y given parameter lambda.\n    \n    Parameters:\n    - lmbda (float): The Poisson rate parameter (must be &gt; 0)\n    - y (array-like): Observed count data\n    \n    Returns:\n    - float: The total log-likelihood value\n    \"\"\"\n    if lmbda &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    \n    y = np.asarray(y)\n    log_likelihood = np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n    return log_likelihood\n\n\n# Log-likelihood function\ndef poisson_log_likelihood(lmbda, y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    y = np.asarray(y)\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\n# Observed count data: number of patents\ny_data = blueprinty[\"patents\"].values\n\n# Range of lambda values to evaluate\nlambda_values = np.linspace(0.1, 20, 200)\n\n# Compute log-likelihoods\nlog_likelihoods = [poisson_log_likelihood(lmbda, y_data) for lmbda in lambda_values]\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood', color='darkblue')\nplt.axvline(x=lambda_values[np.argmax(log_likelihoods)], color='red', linestyle='--', label='MLE')\nplt.title(\"Log-Likelihood of Poisson Model over λ\")\nplt.xlabel(\"λ (Lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTo derive the maximum likelihood estimator (MLE) for ( ), we start with the log-likelihood function of the Poisson model:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\nWe take the first derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nSet the derivative equal to zero to find the maximizer:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\nSolving for ( ):\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\] Thus, the MLE of ( ) is the sample mean, ( {y} ), which matches the expectation of the Poisson parameter:\n\\[\n\\lambda_{\\text{MLE}} = \\bar{y}\n\\]\n\nlambda_mle_analytical = np.mean(y_data)\nprint(f\"Analytical MLE for λ = {lambda_mle_analytical:.4f}\")\n\nAnalytical MLE for λ = 3.6847\n\n\n\n\n\nThe MLE for ( ) is simply the sample mean ( {Y} ), which aligns with our intuition. Since the Poisson distribution has mean ( ), it’s natural that the average observed count provides the\n\nfrom scipy.optimize import minimize_scalar\n\n# Define negative log-likelihood to minimize\ndef neg_poisson_log_likelihood(lmbda, y):\n    return -poisson_log_likelihood(lmbda, y)\n\n# Optimize over a reasonable range of lambda values\nresult = minimize_scalar(\n    fun=neg_poisson_log_likelihood,\n    args=(y_data,),\n    bounds=(0.01, 20),\n    method='bounded'\n)\n\nlambda_mle = result.x\nlambda_mle\nprint(f\"MLE for λ: {lambda_mle:.4f}\")\n\nMLE for λ: 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nTo move beyond the constant-rate Poisson model, I extended the framework to a Poisson regression, allowing the expected number of patents ( _i ) to vary across firms based on observed characteristics. Specifically, I modeled the outcome as:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where } \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis specification assumes that the log of the expected number of patents is a linear function of covariates.\nTo operationalize this, I constructed a design matrix ( X ) that includes firm age, age squared (to capture non-linear effects), a binary indicator for whether the firm is a Blueprinty customer, and dummy variables for region (with one region omitted as the reference group). I then defined a log-likelihood function for the Poisson model and used scipy.optimize.minimize with the BFGS method to estimate the vector of coefficients ( ). I included a clipping step on the linear predictor to avoid numerical overflow when exponentiating large values.\nAfter optimization, I extracted the maximum likelihood estimates for each coefficient as well as the standard errors from the inverse Hessian matrix. These results are presented in a summary table showing the effect of each covariate on the expected number of patents. This regression framework allows us to estimate the adjusted association between Blueprinty usage and innovation outcomes while controlling for other important firm characteristics such as age and location.\n\nfrom scipy.optimize import minimize\n# Standardize age and age_squared\ndf = blueprinty.copy()\ndf[\"age_squared\"] = df[\"age\"] ** 2\n\n# Create model matrix\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX_df = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age\", \"age_squared\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X_df.values\ny = df[\"patents\"].values\n\n# Log-likelihood\ndef neg_log_likelihood(beta, y, X):\n    beta = np.asarray(beta, dtype=np.float64)\n    X = np.asarray(X, dtype=np.float64)\n    y = np.asarray(y, dtype=np.float64)\n\n    eta = np.clip(X @ beta, -20, 20)\n    lambda_i = np.exp(eta)\n    return -np.sum(-lambda_i + y * np.log(lambda_i) - gammaln(y + 1))\n\n\n\n# Re-run optimization safely\nbeta_start = np.zeros(X_matrix.shape[1])\nresult = minimize(\n    fun=neg_log_likelihood,\n    x0=beta_start,\n    args=(y, X_matrix),\n    method=\"BFGS\"\n)\n\n# Extract results\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Present results\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X_df.columns)\n\n\nresults_df \n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509956\n0.193060\n\n\nage\n0.148702\n0.014461\n\n\nage_squared\n-0.002972\n0.000266\n\n\niscustomer\n0.207600\n0.032934\n\n\nNortheast\n0.029159\n0.046770\n\n\nNorthwest\n-0.017578\n0.057230\n\n\nSouth\n0.056567\n0.056244\n\n\nSouthwest\n0.050589\n0.049642\n\n\n\n\n\n\n\nTo estimate the Poisson regression model using a built-in method, I employed the statsmodels library in Python. This approach serves as a validation of the custom maximum likelihood implementation done previously. I began by reconstructing the design matrix ( X ) to include all relevant predictors. Specifically, I included: firm age, the square of firm age (to capture potential non-linear effects), a binary indicator for whether the firm is a Blueprinty customer, and dummy variables for firm region (excluding one region as the baseline category to avoid multicollinearity). I also explicitly added a constant column to allow estimation of an intercept term.\nOnce the design matrix was created, I ensured that all values in both the feature matrix and the outcome vector were of type float64 to meet the requirements of the statsmodels GLM framework. I then specified and fitted a Poisson generalized linear model using the canonical log link function. The outcome variable was patents, which represents the count of patents awarded to each firm.\nThe model was estimated using the GLM() and .fit() methods provided by statsmodels, and the output includes coefficient estimates along with standard errors, z-values, and associated p-values. These results provide a formally estimated Poisson regression model that can be used to interpret how firm characteristics are associated with patent output, while adjusting for potential confounding variables.\n\nimport statsmodels.api as sm\n\n# Standardize age and age_squared\ndf = blueprinty.copy()\ndf[\"age_squared\"] = df[\"age\"] ** 2\n\n# Create model matrix\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX_df = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age\", \"age_squared\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X_df.values\ny_vector = df[\"patents\"].values\n\n# Ensure correct data types\nX_matrix = X_matrix.astype(np.float64)\ny = y.astype(np.float64)\n\n# Fit Poisson GLM\nglm_model = sm.GLM(y, X_matrix, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display results\nglm_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nTue, 06 May 2025\nDeviance:\n2143.3\n\n\nTime:\n20:41:24\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nx1\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nx2\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\nx3\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nx4\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nx5\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nx6\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nx7\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\nInterpreting the Poisson Regression Coefficients:\nThe Poisson regression model estimates the log of the expected number of patents as a function of several firm-level characteristics. The intercept term (beta_0 = -0.510) reflects the log expected number of patents for a firm with all covariates equal to zero — a hypothetical baseline that is not directly interpretable but serves as a reference point.\nThe coefficient on age (x1 = 0.149) is positive and statistically significant, suggesting that older firms tend to produce more patents. Specifically, each one-unit increase in age is associated with an approximate 16% increase in the expected number of patents.\nThe variable iscustomer (x2 = 0.208) is also highly significant and indicates that, on average, Blueprinty customers are expected to produce approximately 23% more patents than non-customers, holding all else constant. This provides strong evidence that using Blueprinty software is positively associated with innovation outcomes.\nThe coefficient on age_squared (x3 = -0.003) is small but negative, suggesting diminishing marginal returns to age — as firms get older, the positive effect of age on patent output begins to level off.\nThe remaining variables capture regional fixed effects, with the reference category being the region omitted from the model (not shown in the table). None of the regional coefficients — region_Northeast (x4), region_Northwest (x5), region_South (x6), and region_Southwest (x7) — are statistically significant, implying that after accounting for firm age and customer status, location does not substantially affect the number of patents.\nOverall, the model suggests that Blueprinty usage and firm age are the primary drivers of variation in patenting activity, with only minor contributions from regional differences.\n\n\n\nTo interpret the practical impact of being a Blueprinty customer, we conduct a counterfactual prediction exercise. We create two hypothetical datasets:\n\nX₀: All firms are treated as non-customers (iscustomer = 0)\nX₁: All firms are treated as customers (iscustomer = 1)\n\nUsing our estimated Poisson regression coefficients, we calculate the predicted number of patents for each firm in both scenarios:\n\\[\n\\hat{Y}_0 = \\exp(X_0 \\hat{\\beta}), \\quad \\hat{Y}_1 = \\exp(X_1 \\hat{\\beta})\n\\]\nThe difference between these predictions represents the model-implied effect of using Blueprinty. Averaging the differences across all firms gives us the estimated treatment effect:\n\\[\n\\text{Average}(\\hat{Y}_1 - \\hat{Y}_0) \\approx 0.79\n\\]\n\n# Predict counterfactuals\nbeta_hat = result.x\nX_0 = X_df.copy()\nX_0[\"iscustomer\"] = 0\nX_1 = X_df.copy()\nX_1[\"iscustomer\"] = 1\n\nX0_matrix = X_0.values\nX1_matrix = X_1.values\n\nX0_matrix  = np.asarray(X0_matrix , dtype=np.float64)\nX1_matrix  = np.asarray(X1_matrix , dtype=np.float64)\n\nlambda_0 = np.exp(np.clip(X0_matrix @ beta_hat, -10, 10))\nlambda_1 = np.exp(np.clip(X1_matrix @ beta_hat, -10, 10))\n\n# Average difference in predicted patent counts\naverage_difference = np.mean(lambda_1 - lambda_0)\naverage_difference\nprint(f\"Average expected effect of using Blueprinty: {average_difference:.4f} patents\")\n\nAverage expected effect of using Blueprinty: 0.7928 patents\n\n\nThe results show that, on average, firms that are modeled as Blueprinty customers are predicted to receive approximately 0.79 more patents than they would as non-customers, holding firm age and region constant. This provides strong evidence that Blueprinty usage is positively associated with increased patent activity. While this estimate does not definitively establish causality due to the observational nature of the data, it does reflect a meaningful association that persists even after adjusting for potential confounders. The finding reinforces the earlier coefficient-based interpretation and supports the hypothesis that Blueprinty’s tools contribute to improved innovation performance."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nTo start the analysis, we began by loading the blueprinty.csv dataset into our environment using pandas: The blueprinty dataset contains information on a sample of firms and includes both outcome and explanatory variables relevant for modeling innovation performance — particularly patent activity. The main variables in the dataset include:\n\npatents: Number of patents awarded to each firm over a 5-year period (count outcome).\nage: Firm age in years.\nregion: Categorical variable representing the firm’s geographical location.\niscustomer: Binary indicator (1 if the firm is a Blueprinty customer, 0 otherwise).\n\n\nimport pandas as pd\n\nblueprinty = pd.read_csv(\"blueprinty.csv\")\n\ndisplay(blueprinty.head())\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\n\nTo investigate whether Blueprinty customers tend to have more patents, we compared the distribution of patents for customers vs. non-customers both visually and numerically. To begin, we grouped the dataset by customer status (iscustomer) and calculated the average number of patents. The results show that Blueprinty customers average 4.13 patents, compared to 3.47 patents for non-customers. This suggests a potential positive association between software usage and innovation outcomes.\nTo visualize this relationship, we created a histogram of patent counts stratified by customer status. Both groups display a right-skewed distribution, typical of count data like patents. However, the histogram shows that Blueprinty customers are more concentrated at higher patent counts, with their distribution clearly shifted to the right relative to non-customers. This provides initial descriptive support for the hypothesis that Blueprinty software may enhance innovation performance.\nThis preliminary finding suggests a potential positive association between customer status and patent count, which will be investigated further using Poisson regression.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nsns.histplot(data=blueprinty, x=\"patents\", hue=\"iscustomer\", binwidth=1, multiple=\"dodge\")\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n\nmeans_by_status = blueprinty.groupby(\"iscustomer\")[\"patents\"].mean().reset_index()\ndisplay(means_by_status)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\npatents\n\n\n\n\n0\n0\n3.473013\n\n\n1\n1\n4.133056\n\n\n\n\n\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\nTo assess whether Blueprinty customers differ systematically from non-customers, we explored variation in geographic region by customer status. This is an important step in evaluating potential confounding variables, which could bias the observed relationship between software usage and patent outcomes.\nWe visualized the regional distribution of firms across customer categories using a bar chart. The results reveal a clear pattern: the Northeast region accounts for a disproportionately high number of Blueprinty customers, while other regions — including the Midwest, Northwest, South, and Southwest — are more heavily populated by non-customers. This suggests that customer status is not randomly assigned with respect to geography.\n\n# Compare average age by customer status\nage = blueprinty.groupby(\"iscustomer\")[\"age\"]\n\n# Compare region distribution by customer status using counts\nregion_counts = pd.crosstab(blueprinty[\"region\"], blueprinty[\"iscustomer\"])\nregion_counts.columns = [\"Non-Customer\", \"Customer\"]\n\nimport matplotlib.pyplot as plt\n\n# Bar plot of regional distributions (counts)\nregion_counts.plot(kind=\"bar\", figsize=(10, 5))\nplt.title(\"Regional Distribution by Customer Status\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Region\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Customer Status\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe implication is that region may influence both the likelihood of adopting Blueprinty software and the number of patents awarded. For instance, firms in the Northeast may have better access to innovation infrastructure, denser professional networks, or different industry concentrations — all of which could affect patenting behavior independently of software usage.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nWe model the number of patents \\(Y_i\\) for each firm \\(i\\) as Poisson-distributed:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe probability mass function of the Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAssuming independent observations for \\(i = 1, 2, \\dots, n\\), the likelihood function for the full sample is:\n\\[\n\\mathcal{L}(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking the natural logarithm, the log-likelihood function becomes:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\nThis log-likelihood will be the basis for estimating the parameter \\(\\lambda\\) using Maximum Likelihood Estimation.\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    \"\"\"\n    Compute the log-likelihood of Poisson-distributed data y given parameter lambda.\n    \n    Parameters:\n    - lmbda (float): The Poisson rate parameter (must be &gt; 0)\n    - y (array-like): Observed count data\n    \n    Returns:\n    - float: The total log-likelihood value\n    \"\"\"\n    if lmbda &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    \n    y = np.asarray(y)\n    log_likelihood = np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n    return log_likelihood\n\n\n# Log-likelihood function\ndef poisson_log_likelihood(lmbda, y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    y = np.asarray(y)\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\n# Observed count data: number of patents\ny_data = blueprinty[\"patents\"].values\n\n# Range of lambda values to evaluate\nlambda_values = np.linspace(0.1, 20, 200)\n\n# Compute log-likelihoods\nlog_likelihoods = [poisson_log_likelihood(lmbda, y_data) for lmbda in lambda_values]\n\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_values, log_likelihoods, label='Log-Likelihood', color='darkblue')\nplt.axvline(x=lambda_values[np.argmax(log_likelihoods)], color='red', linestyle='--', label='MLE')\nplt.title(\"Log-Likelihood of Poisson Model over λ\")\nplt.xlabel(\"λ (Lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTo derive the maximum likelihood estimator (MLE) for ( ), we start with the log-likelihood function of the Poisson model:\n\\[\n\\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right)\n\\]\nWe take the first derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = \\sum_{i=1}^{n} \\left( -1 + \\frac{Y_i}{\\lambda} \\right)\n= -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nSet the derivative equal to zero to find the maximizer:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\nSolving for ( ):\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\] Thus, the MLE of ( ) is the sample mean, ( {y} ), which matches the expectation of the Poisson parameter:\n\\[\n\\lambda_{\\text{MLE}} = \\bar{y}\n\\]\n\nlambda_mle_analytical = np.mean(y_data)\nprint(f\"Analytical MLE for λ = {lambda_mle_analytical:.4f}\")\n\nAnalytical MLE for λ = 3.6847\n\n\n\n\n\nThe MLE for ( ) is simply the sample mean ( {Y} ), which aligns with our intuition. Since the Poisson distribution has mean ( ), it’s natural that the average observed count provides the\n\nfrom scipy.optimize import minimize_scalar\n\n# Define negative log-likelihood to minimize\ndef neg_poisson_log_likelihood(lmbda, y):\n    return -poisson_log_likelihood(lmbda, y)\n\n# Optimize over a reasonable range of lambda values\nresult = minimize_scalar(\n    fun=neg_poisson_log_likelihood,\n    args=(y_data,),\n    bounds=(0.01, 20),\n    method='bounded'\n)\n\nlambda_mle = result.x\nlambda_mle\nprint(f\"MLE for λ: {lambda_mle:.4f}\")\n\nMLE for λ: 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nTo move beyond the constant-rate Poisson model, I extended the framework to a Poisson regression, allowing the expected number of patents ( _i ) to vary across firms based on observed characteristics. Specifically, I modeled the outcome as:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where } \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis specification assumes that the log of the expected number of patents is a linear function of covariates.\nTo operationalize this, I constructed a design matrix ( X ) that includes firm age, age squared (to capture non-linear effects), a binary indicator for whether the firm is a Blueprinty customer, and dummy variables for region (with one region omitted as the reference group). I then defined a log-likelihood function for the Poisson model and used scipy.optimize.minimize with the BFGS method to estimate the vector of coefficients ( ). I included a clipping step on the linear predictor to avoid numerical overflow when exponentiating large values.\nAfter optimization, I extracted the maximum likelihood estimates for each coefficient as well as the standard errors from the inverse Hessian matrix. These results are presented in a summary table showing the effect of each covariate on the expected number of patents. This regression framework allows us to estimate the adjusted association between Blueprinty usage and innovation outcomes while controlling for other important firm characteristics such as age and location.\n\nfrom scipy.optimize import minimize\n# Standardize age and age_squared\ndf = blueprinty.copy()\ndf[\"age_squared\"] = df[\"age\"] ** 2\n\n# Create model matrix\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX_df = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age\", \"age_squared\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X_df.values\ny = df[\"patents\"].values\n\n# Log-likelihood\ndef neg_log_likelihood(beta, y, X):\n    beta = np.asarray(beta, dtype=np.float64)\n    X = np.asarray(X, dtype=np.float64)\n    y = np.asarray(y, dtype=np.float64)\n\n    eta = np.clip(X @ beta, -20, 20)\n    lambda_i = np.exp(eta)\n    return -np.sum(-lambda_i + y * np.log(lambda_i) - gammaln(y + 1))\n\n\n\n# Re-run optimization safely\nbeta_start = np.zeros(X_matrix.shape[1])\nresult = minimize(\n    fun=neg_log_likelihood,\n    x0=beta_start,\n    args=(y, X_matrix),\n    method=\"BFGS\"\n)\n\n# Extract results\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Present results\nresults_df = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": standard_errors\n}, index=X_df.columns)\n\n\nresults_df \n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509956\n0.193060\n\n\nage\n0.148702\n0.014461\n\n\nage_squared\n-0.002972\n0.000266\n\n\niscustomer\n0.207600\n0.032934\n\n\nNortheast\n0.029159\n0.046770\n\n\nNorthwest\n-0.017578\n0.057230\n\n\nSouth\n0.056567\n0.056244\n\n\nSouthwest\n0.050589\n0.049642\n\n\n\n\n\n\n\nTo estimate the Poisson regression model using a built-in method, I employed the statsmodels library in Python. This approach serves as a validation of the custom maximum likelihood implementation done previously. I began by reconstructing the design matrix ( X ) to include all relevant predictors. Specifically, I included: firm age, the square of firm age (to capture potential non-linear effects), a binary indicator for whether the firm is a Blueprinty customer, and dummy variables for firm region (excluding one region as the baseline category to avoid multicollinearity). I also explicitly added a constant column to allow estimation of an intercept term.\nOnce the design matrix was created, I ensured that all values in both the feature matrix and the outcome vector were of type float64 to meet the requirements of the statsmodels GLM framework. I then specified and fitted a Poisson generalized linear model using the canonical log link function. The outcome variable was patents, which represents the count of patents awarded to each firm.\nThe model was estimated using the GLM() and .fit() methods provided by statsmodels, and the output includes coefficient estimates along with standard errors, z-values, and associated p-values. These results provide a formally estimated Poisson regression model that can be used to interpret how firm characteristics are associated with patent output, while adjusting for potential confounding variables.\n\nimport statsmodels.api as sm\n\n# Standardize age and age_squared\ndf = blueprinty.copy()\ndf[\"age_squared\"] = df[\"age\"] ** 2\n\n# Create model matrix\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\nX_df = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age\", \"age_squared\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X_df.values\ny_vector = df[\"patents\"].values\n\n# Ensure correct data types\nX_matrix = X_matrix.astype(np.float64)\ny = y.astype(np.float64)\n\n# Fit Poisson GLM\nglm_model = sm.GLM(y, X_matrix, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display results\nglm_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nTue, 06 May 2025\nDeviance:\n2143.3\n\n\nTime:\n20:41:24\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nx1\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nx2\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\nx3\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nx4\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nx5\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nx6\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nx7\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\nInterpreting the Poisson Regression Coefficients:\nThe Poisson regression model estimates the log of the expected number of patents as a function of several firm-level characteristics. The intercept term (beta_0 = -0.510) reflects the log expected number of patents for a firm with all covariates equal to zero — a hypothetical baseline that is not directly interpretable but serves as a reference point.\nThe coefficient on age (x1 = 0.149) is positive and statistically significant, suggesting that older firms tend to produce more patents. Specifically, each one-unit increase in age is associated with an approximate 16% increase in the expected number of patents.\nThe variable iscustomer (x2 = 0.208) is also highly significant and indicates that, on average, Blueprinty customers are expected to produce approximately 23% more patents than non-customers, holding all else constant. This provides strong evidence that using Blueprinty software is positively associated with innovation outcomes.\nThe coefficient on age_squared (x3 = -0.003) is small but negative, suggesting diminishing marginal returns to age — as firms get older, the positive effect of age on patent output begins to level off.\nThe remaining variables capture regional fixed effects, with the reference category being the region omitted from the model (not shown in the table). None of the regional coefficients — region_Northeast (x4), region_Northwest (x5), region_South (x6), and region_Southwest (x7) — are statistically significant, implying that after accounting for firm age and customer status, location does not substantially affect the number of patents.\nOverall, the model suggests that Blueprinty usage and firm age are the primary drivers of variation in patenting activity, with only minor contributions from regional differences.\n\n\n\nTo interpret the practical impact of being a Blueprinty customer, we conduct a counterfactual prediction exercise. We create two hypothetical datasets:\n\nX₀: All firms are treated as non-customers (iscustomer = 0)\nX₁: All firms are treated as customers (iscustomer = 1)\n\nUsing our estimated Poisson regression coefficients, we calculate the predicted number of patents for each firm in both scenarios:\n\\[\n\\hat{Y}_0 = \\exp(X_0 \\hat{\\beta}), \\quad \\hat{Y}_1 = \\exp(X_1 \\hat{\\beta})\n\\]\nThe difference between these predictions represents the model-implied effect of using Blueprinty. Averaging the differences across all firms gives us the estimated treatment effect:\n\\[\n\\text{Average}(\\hat{Y}_1 - \\hat{Y}_0) \\approx 0.79\n\\]\n\n# Predict counterfactuals\nbeta_hat = result.x\nX_0 = X_df.copy()\nX_0[\"iscustomer\"] = 0\nX_1 = X_df.copy()\nX_1[\"iscustomer\"] = 1\n\nX0_matrix = X_0.values\nX1_matrix = X_1.values\n\nX0_matrix  = np.asarray(X0_matrix , dtype=np.float64)\nX1_matrix  = np.asarray(X1_matrix , dtype=np.float64)\n\nlambda_0 = np.exp(np.clip(X0_matrix @ beta_hat, -10, 10))\nlambda_1 = np.exp(np.clip(X1_matrix @ beta_hat, -10, 10))\n\n# Average difference in predicted patent counts\naverage_difference = np.mean(lambda_1 - lambda_0)\naverage_difference\nprint(f\"Average expected effect of using Blueprinty: {average_difference:.4f} patents\")\n\nAverage expected effect of using Blueprinty: 0.7928 patents\n\n\nThe results show that, on average, firms that are modeled as Blueprinty customers are predicted to receive approximately 0.79 more patents than they would as non-customers, holding firm age and region constant. This provides strong evidence that Blueprinty usage is positively associated with increased patent activity. While this estimate does not definitively establish causality due to the observational nature of the data, it does reflect a meaningful association that persists even after adjusting for potential confounders. The finding reinforces the earlier coefficient-based interpretation and supports the hypothesis that Blueprinty’s tools contribute to improved innovation performance."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nData Cleaning\nBefore modeling, we performed a series of data cleaning steps to ensure the dataset was consistent, complete, and suitable for Poisson regression.\n\nDropped rows with missing values in critical columns:\nWe removed observations where any of the following key identifying or structural variables were missing:\n\nid, days, last_scraped, host_since, room_type, instant_bookable\nThese variables are essential for defining the listing, its availability, and booking characteristics. Missing values in these columns would make the observation unusable for analysis or modeling.\n\nFilled missing values in integer-like columns with rounded mean:\n\nFor bathrooms and bedrooms, we calculated the column mean and rounded it to the nearest integer before filling missing values.\nThis approach preserves the variable’s intended numeric structure while avoiding fractional values that don’t make sense.\n\nFilled missing values in continuous variables with their mean:\n\nFor columns like price, number_of_reviews, review_scores_cleanliness, review_scores_location, and review_scores_value, missing values were filled with the column mean.\nThis standard imputation method avoids dropping additional rows and preserves sample size, while assuming the missingness is relatively random.\n\n\nThese cleaning steps ensure the dataset is free of nulls in relevant variables and suitable for exploratory data analysis and regression modeling.\n\n# Load data\ndf = pd.read_csv(\"airbnb.csv\")\n\ndf.dropna(subset=[\"id\", \"days\", \"last_scraped\", \"host_since\", \"room_type\", \"instant_bookable\"], inplace=True)\n\nfor col in [\"bathrooms\", \"bedrooms\"]:\n    mean_val = df[col].dropna().mean()\n    df[col] = df[col].fillna(int(mean_val))\n\ncols_to_fill = [\n    \"price\", \"number_of_reviews\", \"review_scores_cleanliness\",\n    \"review_scores_location\", \"review_scores_value\"\n]\nfor col in cols_to_fill:\n    mean_val = df[col].dropna().mean()\n    df[col] = df[col].fillna(mean_val)\n\ndf\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n0\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.000000\n9.0000\n9.000000\nf\n\n\n1\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.000000\n10.0000\n9.000000\nf\n\n\n2\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\n9.198376\n9.4136\n9.331587\nf\n\n\n3\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.000000\n9.0000\n9.000000\nf\n\n\n4\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\n1.0\n1.0\n39\n93\n9.000000\n8.0000\n9.000000\nt\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n40623\n40624\n18008937\n266\n4/2/2017\n7/10/2016\nEntire home/apt\n1.5\n2.0\n150\n0\n9.198376\n9.4136\n9.331587\nt\n\n\n40624\n40625\n18009045\n366\n4/2/2017\n4/1/2016\nPrivate room\n1.0\n1.0\n125\n0\n9.198376\n9.4136\n9.331587\nf\n\n\n40625\n40626\n18009065\n587\n4/2/2017\n8/24/2015\nPrivate room\n1.0\n1.0\n80\n0\n9.198376\n9.4136\n9.331587\nt\n\n\n40626\n40627\n18009650\n335\n4/2/2017\n5/2/2016\nPrivate room\n1.0\n1.0\n69\n0\n9.198376\n9.4136\n9.331587\nt\n\n\n40627\n40628\n18009669\n1\n4/2/2017\n4/1/2017\nEntire home/apt\n1.0\n1.0\n115\n0\n9.198376\n9.4136\n9.331587\nt\n\n\n\n\n40593 rows × 14 columns\n\n\n\n\n\nExploratory Analysis: Distribution of Room Types\nTo understand the types of listings available on Airbnb in New York City, we visualized the distribution of the room_type variable using a bar chart.\n\nroom_counts = df[\"room_type\"].value_counts()\n\nplt.figure(figsize=(8, 5))\nsns.barplot(x=room_counts.index, y=room_counts.values)\nplt.title(\"Distribution of Room Types\")\nplt.xlabel(\"Room Type\")\nplt.ylabel(\"Number of Listings\")\nplt.grid(axis='y', linestyle='--', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\nThe distribution of room types among Airbnb listings in New York City reveals that the vast majority of listings are either entire homes/apartments or private rooms, each category accounting for nearly 20,000 listings. In contrast, shared rooms represent only a small fraction of the market, indicating that such accommodations are relatively uncommon. This pattern suggests that most Airbnb hosts cater to travelers who prioritize privacy, either through full-unit rentals or private sleeping spaces. From a modeling perspective, room type is likely to be an informative categorical variable, as it may capture meaningful variation in factors such as price, demand, and guest experience. Accordingly, it should be included in any predictive models of review counts or booking behavior. ### Exploratory Analysis: Distribution of Airbnb Prices\nTo explore pricing across Airbnb listings, we examined the distribution of the price variable using a histogram with a kernel density estimate (KDE) overlay.\n\nprint(\"Summary statistics for 'price':\")\nprint(df[\"price\"].describe())\n\nplt.figure(figsize=(10, 6))\nsns.histplot(df[\"price\"], bins=50, kde=True)\nplt.title(\"Distribution of Airbnb Prices\")\nplt.xlabel(\"Price\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\nSummary statistics for 'price':\ncount    40593.000000\nmean       144.762250\nstd        210.703908\nmin         10.000000\n25%         70.000000\n50%        100.000000\n75%        170.000000\nmax      10000.000000\nName: price, dtype: float64\n\n\n\n\n\n\n\n\n\nInsights:\n\nThe distribution of prices is strongly right-skewed, with the vast majority of listings priced below $500 per night.\nThe x-axis stretches to $10,000, which suggests the presence of extreme outliers — high-priced luxury listings that are rare but heavily affect the scale of the histogram.\nThe mode of the distribution (most common price point) appears to be under $200.\n\nThese outliers could heavily influence the mean and regression results. It may be useful to filter or winsorize extreme price values in further analysis.\nThe initial histogram of Airbnb prices revealed a highly skewed distribution, with some listings priced as high as $10,000 per night. These extreme values are likely outliers and can disproportionately influence summary statistics and regression models.\nTo address this, we quantified and removed listings with prices above $1,000:\n\nhigh_price_count = (df[\"price\"] &gt; 1000).sum()\nprint(f\"Number of listings with price &gt; 1000: {high_price_count}\")\ndf = df[df[\"price\"] &lt;= 1000]\n\nNumber of listings with price &gt; 1000: 146\n\n\nThere were 146 listings identified and removed based on the $1,000 price threshold. These represent high-end, likely luxury properties that are not representative of the broader NYC Airbnb market. As the number of listings only contributes to a significantly small amount of the total dataset, by excluding them, we improve model stability, reduce variance, and allow clearer interpretation of pricing trends among typical listings.\nTherefore, we revised on the plot as below:\n\nplt.figure(figsize=(10, 6))\nsns.histplot(df[\"price\"], bins=50, kde=True)\nplt.title(\"Distribution of Airbnb Prices\")\nplt.xlabel(\"Price\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nInsights: - The revised distribution shows a more focused and realistic range of prices, with most listings falling between $50 and $300 per night. - The modal price range (most frequent) appears to be around $100–$150. - Although still right-skewed, the distribution is much less extreme and more analytically useful for summary statistics and regression modeling. - This adjustment helps ensure that further statistical models are not dominated by rare high-end listings.\n\n\nDistribution of Number of Reviews\nTo explore how frequently listings are reviewed on Airbnb, we examined the distribution of the number_of_reviews variable:\n\nprint(\"Summary statistics for 'number_of_reviews':\")\nprint(df[\"number_of_reviews\"].describe())\n\nplt.figure(figsize=(10, 6))\nsns.histplot(df[\"number_of_reviews\"], bins=50, kde=True)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\nSummary statistics for 'number_of_reviews':\ncount    40447.000000\nmean        15.940119\nstd         29.282452\nmin          0.000000\n25%          1.000000\n50%          4.000000\n75%         17.000000\nmax        421.000000\nName: number_of_reviews, dtype: float64\n\n\n\n\n\n\n\n\n\nInsights:\n\nThe distribution is highly right-skewed, with the majority of listings having fewer than 25 reviews.\nThere are many listings with zero reviews, and only a small number exceed 100 reviews.\nThe long tail suggests that a handful of properties are booked and reviewed far more frequently than the rest — likely due to factors such as location, price, or host quality.\nThe shape of this distribution supports the use of Poisson regression for modeling count data like number_of_reviews, possibly with adjustments for overdispersion or zero inflation if needed.\n\nFrom the histogram of number_of_reviews, we observed that the distribution is extremely right-skewed. While most listings receive relatively few reviews, there are a small number of listings with exceptionally high review counts.\nTo quantify these potential outliers, we examined how many listings had more than 200 reviews:\n\nhigh_review_count = (df[\"number_of_reviews\"] &gt; 100).sum()\nprint(f\"Number of listings with more than 200 reviews: {high_review_count}\")\n\nNumber of listings with more than 200 reviews: 1085\n\n\nOnly 1,085 listings have more than 200 reviews, out of tens of thousands of entries. This confirms that a very small proportion of listings receive a disproportionately large share of reviews.\nThese outliers can heavily influence mean-based metrics and potentially distort model results.Given the extreme right skew of the number_of_reviews variable, we filtered the dataset to only include listings with 100 or fewer reviews — the range where most of the data lies. This allows us to better observe the underlying structure and distribution.\n\nplt.figure(figsize=(10, 6))\nsubset = df[df[\"number_of_reviews\"] &lt;= 100]\nsns.histplot(subset[\"number_of_reviews\"], bins=100, kde=True)\nplt.title(\"Distribution of Number of Reviews (0–100)\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nInsights:\n\nThe plot reveals a very steep drop-off in review counts: most listings have fewer than 10 reviews, with the most common value being zero.\nThere’s a long but thinner tail between 10 and 100 reviews.\nThese results reinforce that review activity is concentrated among a small number of frequently booked listings, while most receive minimal engagement.\nThis justifies modeling number_of_reviews as count data, likely using Poisson or negative binomial regression.\n\n\n\nPrice vs. Review Score (Value)\nTo explore whether customer-perceived “value for money” correlates with listing price, we visualized the relationship between price and review_scores_value using a scatter plot:\nWe plotted each listing’s price against its corresponding value review score (1–10 scale). - alpha=0.4 was used to reduce overplotting and make dense clusters easier to interpret. - This allows us to visually inspect whether higher prices are associated with lower perceived value, or if highly rated listings tend to be more expensive\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x=\"review_scores_value\", y=\"price\", alpha=0.4)\nplt.title(\"Price vs Review Score (Value)\")\nplt.xlabel(\"Review Score: Value\")\nplt.ylabel(\"Price\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nInsights:\nThe scatter plot of price against the review score for value reveals several key patterns. First, the vast majority of listings are concentrated at high review scores, particularly between 8 and 10, indicating that most guests perceive their stays as good value for money. Despite this clustering, prices vary widely within each review score category — especially at higher scores — suggesting that higher price does not necessarily equate to lower perceived value. Interestingly, some listings with very high prices (e.g., above $800) still receive value scores of 9 or 10, indicating that guests may perceive expensive listings as worthwhile if the quality or amenities justify the cost. Conversely, listings with lower value scores (below 6) are fewer in number and tend to be more scattered in price, but still include a few high-priced outliers, suggesting occasional misalignments between pricing and guest expectations. Overall, the relationship between price and value rating appears weak and non-linear."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#modeling-number-of-reviews-with-poisson-regression",
    "href": "blog/project2/hw2_questions.html#modeling-number-of-reviews-with-poisson-regression",
    "title": "Poisson Regression Examples",
    "section": "Modeling Number of Reviews with Poisson Regression",
    "text": "Modeling Number of Reviews with Poisson Regression\nTo model the number of reviews — treated as a count variable and proxy for demand or bookings — we used a Poisson regression model, which is appropriate for count data.\n\nVariables Included\nWe modeled number_of_reviews as a function of the following explanatory variables:\n\ndays: How long the listing has been active\nbathrooms, bedrooms: Size and features of the listing\nprice: Cost per night\nreview_scores_cleanliness, review_scores_location, review_scores_value: Guest satisfaction metrics\ninstant_bookable: Binary indicator of whether the listing can be booked instantly\nroom_type: Categorical variable (converted to dummy variables with one dropped for reference)\n\n\n\nModel Specification and Estimation\n\ny = df[\"number_of_reviews\"]\n\ncolumns_needed_for_x = [\n    \"days\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]\nX = df[columns_needed_for_x].copy()\nX[\"instant_bookable\"] = X[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\nX = pd.get_dummies(X, columns=[\"room_type\"], drop_first=True)\n\n\nX = sm.add_constant(X)\nX = X.astype(float)\n\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\n\npoisson_model.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n40447\n\n\nModel:\nGLM\nDf Residuals:\n40436\n\n\nModel Family:\nPoisson\nDf Model:\n10\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-6.5524e+05\n\n\nDate:\nTue, 06 May 2025\nDeviance:\n1.1868e+06\n\n\nTime:\n20:41:25\nPearson chi2:\n1.73e+06\n\n\nNo. Iterations:\n6\nPseudo R-squ. (CS):\n0.9750\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n2.7445\n0.018\n149.240\n0.000\n2.708\n2.781\n\n\ndays\n0.0006\n1.83e-06\n346.349\n0.000\n0.001\n0.001\n\n\nbathrooms\n-0.1045\n0.004\n-26.785\n0.000\n-0.112\n-0.097\n\n\nbedrooms\n0.0875\n0.002\n41.740\n0.000\n0.083\n0.092\n\n\nprice\n-0.0004\n1.57e-05\n-27.507\n0.000\n-0.000\n-0.000\n\n\nreview_scores_cleanliness\n0.1429\n0.002\n83.242\n0.000\n0.140\n0.146\n\n\nreview_scores_location\n-0.1036\n0.002\n-57.461\n0.000\n-0.107\n-0.100\n\n\nreview_scores_value\n-0.1146\n0.002\n-55.633\n0.000\n-0.119\n-0.111\n\n\ninstant_bookable\n0.5068\n0.003\n174.621\n0.000\n0.501\n0.512\n\n\nroom_type_Private room\n-0.0936\n0.003\n-31.611\n0.000\n-0.099\n-0.088\n\n\nroom_type_Shared room\n-0.2320\n0.009\n-26.641\n0.000\n-0.249\n-0.215\n\n\n\n\n\nInsights:\n\ndays: Each additional day a listing has been active is associated with a small but statistically significant increase in expected reviews (( = 0.0006 )). This makes intuitive sense: the longer a listing is online, the more reviews it can accumulate.\nbathrooms: Surprisingly, listings with more bathrooms are associated with fewer reviews (beta = -0.1044). This might reflect that larger, higher-end units cater to longer stays or smaller guest segments.\nbedrooms: More bedrooms are positively associated with review count (beta = 0.0869), which aligns with the idea that larger listings can accommodate more guests and attract more traffic.\nprice: The effect of price is negative but small (beta = -0.00004). Higher-priced listings receive slightly fewer reviews, all else equal, likely due to reduced accessibility or demand.\nReview scores:\n\nreview_scores_cleanliness has a strong positive association with number of reviews (beta = 0.1425), suggesting that clean listings attract more bookings and positive feedback loops.\nreview_scores_location and review_scores_value both have significant negative associations. This may reflect non-linear effects or higher expectations associated with high scores in these categories.\n\ninstant_bookable: One of the strongest predictors. Listings that allow instant booking receive significantly more reviews (beta = 0.5066 ), likely because they reduce friction for potential guests.\nroom_type:\n\nPrivate rooms receive fewer reviews than entire homes (beta = -0.0918 ).\nShared rooms receive even fewer (beta = -0.2298).\nThis confirms that guests prefer private or entire accommodations — especially for longer or more frequent stays.\n\n\nOverall, the model fits well (Pseudo R² ≈ 0.975), and the signs and magnitudes of coefficients are consistent with expectations from the Airbnb marketplace."
  }
]