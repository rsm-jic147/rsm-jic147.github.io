<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jiayin Chen">
<meta name="dcterms.date" content="2025-06-05">

<title>K-Means Analysis – Jiayin’s Website</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Jiayin’s Website</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resume.html"> 
<span class="menu-text">Resume</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#k-means-analysis" id="toc-k-means-analysis" class="nav-link active" data-scroll-target="#k-means-analysis">1. K-Means Analysis</a>
  <ul class="collapse">
  <li><a href="#mathematical-formulation" id="toc-mathematical-formulation" class="nav-link" data-scroll-target="#mathematical-formulation">Mathematical Formulation</a></li>
  <li><a href="#objective-function" id="toc-objective-function" class="nav-link" data-scroll-target="#objective-function">Objective Function</a></li>
  </ul></li>
  <li><a href="#k-means-clustering-on-the-palmer-penguins-dataset" id="toc-k-means-clustering-on-the-palmer-penguins-dataset" class="nav-link" data-scroll-target="#k-means-clustering-on-the-palmer-penguins-dataset">K-Means Clustering on the Palmer Penguins Dataset</a>
  <ul class="collapse">
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data">Data</a></li>
  <li><a href="#evaluating-optimal-cluster-count-using-wcss-and-silhouette-scores" id="toc-evaluating-optimal-cluster-count-using-wcss-and-silhouette-scores" class="nav-link" data-scroll-target="#evaluating-optimal-cluster-count-using-wcss-and-silhouette-scores">Evaluating Optimal Cluster Count Using WCSS and Silhouette Scores</a></li>
  </ul></li>
  <li><a href="#k-nearest-neighbors" id="toc-k-nearest-neighbors" class="nav-link" data-scroll-target="#k-nearest-neighbors">2. K Nearest Neighbors</a>
  <ul class="collapse">
  <li><a href="#mathematical-formulation-1" id="toc-mathematical-formulation-1" class="nav-link" data-scroll-target="#mathematical-formulation-1">Mathematical Formulation</a></li>
  </ul></li>
  <li><a href="#generating-a-synthetic-dataset-for-knn-classification" id="toc-generating-a-synthetic-dataset-for-knn-classification" class="nav-link" data-scroll-target="#generating-a-synthetic-dataset-for-knn-classification">Generating a Synthetic Dataset for KNN Classification</a>
  <ul class="collapse">
  <li><a href="#visualizing-the-synthetic-classification-dataset" id="toc-visualizing-the-synthetic-classification-dataset" class="nav-link" data-scroll-target="#visualizing-the-synthetic-classification-dataset">Visualizing the Synthetic Classification Dataset</a></li>
  <li><a href="#generating-a-test-dataset" id="toc-generating-a-test-dataset" class="nav-link" data-scroll-target="#generating-a-test-dataset">Generating a Test Dataset</a></li>
  <li><a href="#implementing-k-nearest-neighbors-from-scratch" id="toc-implementing-k-nearest-neighbors-from-scratch" class="nav-link" data-scroll-target="#implementing-k-nearest-neighbors-from-scratch">Implementing K-Nearest Neighbors from Scratch</a></li>
  <li><a href="#evaluating-knn-accuracy-across-varying-values-of-k" id="toc-evaluating-knn-accuracy-across-varying-values-of-k" class="nav-link" data-scroll-target="#evaluating-knn-accuracy-across-varying-values-of-k">Evaluating KNN Accuracy Across Varying Values of k</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">K-Means Analysis</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jiayin Chen </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 5, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="k-means-analysis" class="level2">
<h2 class="anchored" data-anchor-id="k-means-analysis">1. K-Means Analysis</h2>
<p>K-Means is an unsupervised machine learning algorithm used for clustering data into distinct groups (called clusters) based on similarity. The objective is to partition a dataset into K clusters in which each data point belongs to the cluster with the nearest mean (also called the centroid).</p>
<section id="mathematical-formulation" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-formulation">Mathematical Formulation</h3>
<ul>
<li><span class="math inline">\(X = \{x_1, x_2, ..., x_n\} \subset \mathbb{R}^d\)</span> be the set of <span class="math inline">\(n\)</span> data points in <span class="math inline">\(d\)</span>-dimensional space.</li>
<li><span class="math inline">\(K\)</span> be the number of clusters.</li>
<li><span class="math inline">\(\mu_1, \mu_2, ..., \mu_K \in \mathbb{R}^d\)</span> be the <strong>centroids</strong> of the clusters.</li>
<li><span class="math inline">\(C_k\)</span> be the set of indices of data points assigned to cluster <span class="math inline">\(k\)</span>.</li>
</ul>
</section>
<section id="objective-function" class="level3">
<h3 class="anchored" data-anchor-id="objective-function">Objective Function</h3>
<p>The K-Means algorithm minimizes the <strong>within-cluster sum of squared distances</strong> (WCSS):</p>
<p><span class="math display">\[
\underset{C, \mu}{\arg\min} \sum_{k=1}^{K} \sum_{i \in C_k} \| x_i - \mu_k \|^2
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\| x_i - \mu_k \|^2\)</span> is the squared Euclidean distance between point <span class="math inline">\(x_i\)</span> and centroid <span class="math inline">\(\mu_k\)</span>.</li>
<li>The goal is to find the cluster assignments <span class="math inline">\(C\)</span> and centroids <span class="math inline">\(\mu_k\)</span> that minimize this value.</li>
</ul>
</section>
</section>
<section id="k-means-clustering-on-the-palmer-penguins-dataset" class="level2">
<h2 class="anchored" data-anchor-id="k-means-clustering-on-the-palmer-penguins-dataset">K-Means Clustering on the Palmer Penguins Dataset</h2>
<section id="data" class="level3">
<h3 class="anchored" data-anchor-id="data">Data</h3>
<p>In this first analysis, we have been provided a dataset <code>palmer_penguins.csv</code>. To start the analysis, we began by loading the dataset into our environment using <code>pandas</code>.</p>
<p>The dataset provides detailed biological measurements and classification information for a sample of penguins across different islands. It is commonly used in data science education for exploratory analysis and classification modeling. The main variables in the dataset include:</p>
<ul>
<li><code>species</code>: The species of the penguin (e.g., Adelie, Chinstrap, Gentoo).</li>
<li><code>island</code>: The island where the penguin was observed (e.g., Torgersen, Biscoe, Dream).</li>
<li><code>bill_length_mm</code>: The length of the penguin’s bill (in millimeters).</li>
<li><code>bill_depth_mm</code>: The depth of the penguin’s bill (in millimeters).</li>
<li><code>flipper_length_mm</code>: The length of the penguin’s flippers (in millimeters).</li>
<li><code>body_mass_g</code>: The body mass of the penguin (in grams).</li>
<li><code>sex</code>: The sex of the penguin (male or female).</li>
<li><code>year</code>: The year in which the observation was recorded.</li>
</ul>
<div id="8ae85f9d" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"palmer_penguins.csv"</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>display(df.head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">species</th>
<th data-quarto-table-cell-role="th">island</th>
<th data-quarto-table-cell-role="th">bill_length_mm</th>
<th data-quarto-table-cell-role="th">bill_depth_mm</th>
<th data-quarto-table-cell-role="th">flipper_length_mm</th>
<th data-quarto-table-cell-role="th">body_mass_g</th>
<th data-quarto-table-cell-role="th">sex</th>
<th data-quarto-table-cell-role="th">year</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Adelie</td>
<td>Torgersen</td>
<td>39.1</td>
<td>18.7</td>
<td>181</td>
<td>3750</td>
<td>male</td>
<td>2007</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Adelie</td>
<td>Torgersen</td>
<td>39.5</td>
<td>17.4</td>
<td>186</td>
<td>3800</td>
<td>female</td>
<td>2007</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Adelie</td>
<td>Torgersen</td>
<td>40.3</td>
<td>18.0</td>
<td>195</td>
<td>3250</td>
<td>female</td>
<td>2007</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Adelie</td>
<td>Torgersen</td>
<td>36.7</td>
<td>19.3</td>
<td>193</td>
<td>3450</td>
<td>female</td>
<td>2007</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Adelie</td>
<td>Torgersen</td>
<td>39.3</td>
<td>20.6</td>
<td>190</td>
<td>3650</td>
<td>male</td>
<td>2007</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>In this section, we implement the K-Means clustering algorithm from scratch to explore how it partitions penguins based on two selected features: <strong>bill length</strong> and <strong>flipper length</strong>. By visualizing the intermediate steps of the algorithm — including the initial centroid placement, point assignment, and centroid updates — we gain insight into how K-Means iteratively refines cluster boundaries.</p>
<p>To evaluate the performance and accuracy of our custom implementation, we also compare the results to those obtained using Python’s built-in <code>KMeans</code> function from the <code>scikit-learn</code> library. This comparison helps validate our understanding of the algorithm and highlights any differences in cluster assignments or centroid convergence behavior.</p>
<p>In this implementation, we apply the K-Means clustering algorithm to the Palmer Penguins dataset using two numeric features: <code>bill_length_mm</code> and <code>flipper_length_mm</code>. After removing missing values, we randomly initialize three centroids (since <span class="math inline">\(k=3\)</span>) from the data points using a fixed random seed for re</p>
<div id="2e5e287f" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select the two features for clustering</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> df[[<span class="st">'bill_length_mm'</span>, <span class="st">'flipper_length_mm'</span>]].dropna().values</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set number of clusters</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Fix random seed and select common initial centroids</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">88</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>initial_indices <span class="op">=</span> np.random.choice(data.shape[<span class="dv">0</span>], size<span class="op">=</span>k, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>initial_centroids <span class="op">=</span> data[initial_indices]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> euclidean_distance(a, b):</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.linalg.norm(a <span class="op">-</span> b)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kmeans(data, initial_centroids, max_iters<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    centroids <span class="op">=</span> initial_centroids.copy()</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> np.zeros(<span class="bu">len</span>(data), dtype<span class="op">=</span><span class="bu">int</span>)  <span class="co"># initialize labels array</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        clusters <span class="op">=</span> [[] <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(centroids))]</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, point <span class="kw">in</span> <span class="bu">enumerate</span>(data):</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>            distances <span class="op">=</span> [np.linalg.norm(point <span class="op">-</span> c) <span class="cf">for</span> c <span class="kw">in</span> centroids]</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>            cluster_idx <span class="op">=</span> np.argmin(distances)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>            clusters[cluster_idx].append(point)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>            labels[idx] <span class="op">=</span> cluster_idx  <span class="co"># store label assignment</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        previous_centroids <span class="op">=</span> centroids.copy()</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        centroids <span class="op">=</span> np.array([np.mean(cluster, axis<span class="op">=</span><span class="dv">0</span>) <span class="cf">if</span> cluster <span class="cf">else</span> previous_centroids[i] </span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>                              <span class="cf">for</span> i, cluster <span class="kw">in</span> <span class="bu">enumerate</span>(clusters)])</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.allclose(centroids, previous_centroids):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> centroids, clusters, labels</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    centroids, clusters, _ <span class="op">=</span> kmeans(data, initial_centroids)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="be9a6a96" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hw4_questions_files/figure-html/cell-4-output-1.png" width="593" height="449" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hw4_questions_files/figure-html/cell-4-output-2.png" width="593" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The resulting scatter plots illustrate the clustering output from both a manual implementation of the K-Means algorithm and the built-in <code>KMeans</code> function from <code>scikit-learn</code>, applied to the penguin dataset using <code>bill_length_mm</code> and <code>flipper_length_mm</code> as features. In both plots, the penguins are separated into three distinct clusters, each marked by different colors, with centroids shown as prominent markers — black “X” symbols in the custom implementation and red “X” markers in the <code>scikit-learn</code> version. The clusters visually correspond to different size profiles: one group with smaller bills and flippers, another with intermediate measurements, and a third with larger bills and longer flippers. The similarity in results between the two methods validates the correctness of the manual algorithm, while the <code>scikit-learn</code> version also demonstrates the efficiency and clarity of using optimized library tools. Both approaches suggest that the selected features effectively capture natural groupings within the data and that the algorithm converges to stable and meaningful cluster structures.</p>
</section>
<section id="evaluating-optimal-cluster-count-using-wcss-and-silhouette-scores" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-optimal-cluster-count-using-wcss-and-silhouette-scores">Evaluating Optimal Cluster Count Using WCSS and Silhouette Scores</h3>
<p>In the following section, we assess the optimal number of clusters for the penguin dataset by calculating two key evaluation metrics: the <strong>within-cluster sum of squares (WCSS)</strong> and the <strong>silhouette score</strong>. Using built-in functions from <code>scikit-learn</code>, we compute these metrics for values of <span class="math inline">\(K\)</span> ranging from 2 to 7. WCSS helps quantify the compactness of clusters, while the silhouette score provides a measure of how well each data point fits within its assigned cluster relative to other clusters. We visualize both metrics across the range of <span class="math inline">\(K\)</span> values to determine the number of clusters that offers the best balance between cohesion and separation. The goal is to identify the most appropriate cluster count supported by both metrics.</p>
<p>To evaluate the optimal number of clusters for K-Means clustering, we computed two key metrics — the <strong>within-cluster sum of squares (WCSS)</strong> and the <strong>silhouette score</strong> — across a range of cluster values from <span class="math inline">\(k = 2\)</span> to <span class="math inline">\(k = 7\)</span>. For each value of <span class="math inline">\(k\)</span>, we applied both our <strong>custom implementation</strong> of the K-Means algorithm and the <strong><code>scikit-learn</code> KMeans function</strong>, ensuring consistent initial centroids by fixing the random seed. The <code>calculate_wcss</code> function explicitly measures the compactness of each cluster by summing the squared distances_</p>
<div id="985f4cd2" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_wcss(clusters, centroids):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(np.<span class="bu">sum</span>((np.array(cluster) <span class="op">-</span> centroid) <span class="op">**</span> <span class="dv">2</span>) </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>               <span class="cf">for</span> cluster, centroid <span class="kw">in</span> <span class="bu">zip</span>(clusters, centroids))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize results</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop over k values</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">8</span>):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">88</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    indices <span class="op">=</span> np.random.choice(data.shape[<span class="dv">0</span>], size<span class="op">=</span>k, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    initial_centroids <span class="op">=</span> data[indices]</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Custom kmeans</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    custom_centroids, custom_clusters, custom_labels <span class="op">=</span> kmeans(data, initial_centroids)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    custom_wcss <span class="op">=</span> <span class="bu">sum</span>(np.<span class="bu">sum</span>((np.array(cluster) <span class="op">-</span> centroid)<span class="op">**</span><span class="dv">2</span>) </span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>                      <span class="cf">for</span> cluster, centroid <span class="kw">in</span> <span class="bu">zip</span>(custom_clusters, custom_centroids))</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    custom_silhouette <span class="op">=</span> silhouette_score(data, custom_labels)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> k <span class="op">==</span> <span class="dv">4</span>: </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        custom_test_label <span class="op">=</span> custom_labels</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sklearn kmeans with same centroids</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    kmeans_model <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, init<span class="op">=</span>initial_centroids, n_init<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    sk_labels <span class="op">=</span> kmeans_model.fit_predict(data)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    sk_wcss <span class="op">=</span> kmeans_model.inertia_</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    sk_silhouette <span class="op">=</span> silhouette_score(data, sk_labels)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> k <span class="op">==</span> <span class="dv">4</span>: </span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        sk_test_label <span class="op">=</span> sk_labels</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    results.append({</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        <span class="st">"k"</span>: k,</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        <span class="st">"custom_wcss"</span>: custom_wcss,</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="st">"custom_silhouette"</span>: custom_silhouette,</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="st">"sklearn_wcss"</span>: sk_wcss,</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>        <span class="st">"sklearn_silhouette"</span>: sk_silhouette</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame for plotting</span></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>results_df <span class="op">=</span> pd.DataFrame(results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="9cf1cb90" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hw4_questions_files/figure-html/cell-6-output-1.png" width="950" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The evaluation metrics plotted across different values of <span class="math inline">\(k\)</span> provide clear guidance on selecting the optimal number of clusters for the penguin dataset. The <strong>Within-Cluster Sum of Squares (WCSS)</strong> plot shows a typical “elbow” shape — WCSS decreases sharply from <span class="math inline">\(k=2\)</span> to <span class="math inline">\(k=3\)</span>, and then continues to decline more gradually from <span class="math inline">\(k=4\)</span> onward. This indicates diminishing returns in intra-cluster compactness with increasing <span class="math inline">\(k\)</span>, and the inflection point at <span class="math inline">\(k=3\)</span> suggests a natural clustering structure.</p>
<p>The <strong>silhouette score</strong>, which measures the quality of clustering in terms of both cohesion and separation, is highest at <span class="math inline">\(k=2\)</span> and then drops notably for higher values. While <span class="math inline">\(k=2\)</span> offers the best silhouette score, it likely oversimplifies the structure by grouping too broadly. At <span class="math inline">\(k=3\)</span>, the silhouette score remains reasonably high and represents a better balance between compactness and distinct separation, especially when cross-referenced with the WCSS elbow.</p>
<p>Given the combined interpretation of both metrics, <strong><span class="math inline">\(k=3\)</span> emerges as the most appropriate number of clusters</strong>. It offers a meaningful segmentation of the data with strong internal cohesion and clear boundaries between clusters, as confirmed by both the elbow method and the silhouette analysis.</p>
</section>
</section>
<section id="k-nearest-neighbors" class="level2">
<h2 class="anchored" data-anchor-id="k-nearest-neighbors">2. K Nearest Neighbors</h2>
<p><strong>K-Nearest Neighbors (KNN)</strong> is a simple, non-parametric, instance-based learning algorithm used for both classification and regression. In classification, the algorithm assigns a data point to the class most common among its <span class="math inline">\(k\)</span> nearest neighbors in the feature space. In regression, it predicts a value by averaging the values of the <span class="math inline">\(k\)</span> closest neighbors.</p>
<p>Given a new input vector <span class="math inline">\(x_{\text{new}} \in \mathbb{R}^d\)</span>, the algorithm works as follows:</p>
<ol type="1">
<li>Compute the distance between <span class="math inline">\(x_{\text{new}}\)</span> and all training samples <span class="math inline">\(\{x_1, x_2, ..., x_n\}\)</span>.</li>
<li>Select the <span class="math inline">\(k\)</span> training samples closest to <span class="math inline">\(x_{\text{new}}\)</span> using a distance metric (commonly Euclidean distance).</li>
<li>For classification:
<ul>
<li>Assign the label that appears most frequently among the <span class="math inline">\(k\)</span> neighbors.</li>
</ul></li>
<li>For regression:
<ul>
<li>Predict the target as the mean (or weighted mean) of the target values of the <span class="math inline">\(k\)</span> neighbors.</li>
</ul></li>
</ol>
<section id="mathematical-formulation-1" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-formulation-1">Mathematical Formulation</h3>
<p>Let <span class="math inline">\(D(x_i, x_{\text{new}})\)</span> denote the distance between a training point <span class="math inline">\(x_i\)</span> and a query point <span class="math inline">\(x_{\text{new}}\)</span> (commonly:</p>
<p><span class="math display">\[
D(x_i, x_{\text{new}}) = \|x_i - x_{\text{new}}\|_2 = \sqrt{\sum_{j=1}^{d}(x_{ij} - x_{\text{new},j})^2}
\]</span></p>
<p>For classification, the predicted label is:</p>
<p><span class="math display">\[
\hat{y}_{\text{new}} = \text{mode} \left( \{ y_i \mid x_i \in \mathcal{N}_k(x_{\text{new}}) \} \right)
\]</span></p>
<p>For regression:</p>
<p><span class="math display">\[
\hat{y}_{\text{new}} = \frac{1}{k} \sum_{x_i \in \mathcal{N}_k(x_{\text{new}})} y_i
\]</span></p>
<p>Where: - <span class="math inline">\(\mathcal{N}_k(x_{\text{new}})\)</span> is the set of <span class="math inline">\(k\)</span> nearest neighbors of <span class="math inline">\(x_{\text{new}}\)</span> - <span class="math inline">\(\text{mode}(\cdot)\)</span> returns the most frequent class - <span class="math inline">\(y_i\)</span> is the target value or label associated with point <span class="math inline">\(x_i\)</span></p>
<p>KNN is intuitive and effective for low-dimensional data, but its performance can degrade in high-dimensional spaces due to the <strong>curse of dimensionality</strong>.</p>
</section>
</section>
<section id="generating-a-synthetic-dataset-for-knn-classification" class="level2">
<h2 class="anchored" data-anchor-id="generating-a-synthetic-dataset-for-knn-classification">Generating a Synthetic Dataset for KNN Classification</h2>
<p>In this section, we generate a synthetic dataset specifically designed to test the behavior of the K-Nearest Neighbors (KNN) algorithm. The dataset contains two numerical features, <code>x1</code> and <code>x2</code>, along with a binary target variable <code>y</code>. The label assignment is determined by a non-linear boundary defined by a sine function — specifically, whether <code>x2</code> lies above or below a wiggly threshold curve of the form <span class="math inline">\(\sin(x_1)\)</span>. This creates a clearly separable yet non-linear pattern, making it ideal for evaluating the flexibility of KNN in capturing complex decision boundaries. The synthetic nature of the data allows us to precisely control and visualize how the algorithm responds to changes in <span class="math inline">\(k\)</span> and model complexity.</p>
<div id="d993c2d3" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>boundary <span class="op">=</span> np.sin(<span class="dv">4</span> <span class="op">*</span> x1) <span class="op">+</span> x1</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (x2 <span class="op">&gt;</span> boundary).astype(<span class="bu">int</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the DataFrame and convert y to categorical afterward</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>dat <span class="op">=</span> pd.DataFrame({<span class="st">'x1'</span>: x1, <span class="st">'x2'</span>: x2, <span class="st">'y'</span>: y})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="visualizing-the-synthetic-classification-dataset" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-synthetic-classification-dataset">Visualizing the Synthetic Classification Dataset</h3>
<p>To better understand the structure of the synthetic dataset, we create a scatter plot with <code>x1</code> on the horizontal axis and <code>x2</code> on the vertical axis. Each point is colored according to its binary class label <code>y</code>, visually highlighting the decision boundary. The plot reveals a distinct wavy separation between the two classes, corresponding to the sine function used during data generation. Optionally, the true underlying boundary — defined by the curve <span class="math inline">\(x_2 = \sin(x_1)\)</span> — can also be overlaid on the plot to illustrate how well the data reflects the intended non-linear classification pattern. This visualization serves as a useful diagnostic for interpreting model fit and classification complexity in subsequent KNN analyses.</p>
<div id="866bd554" class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hw4_questions_files/figure-html/cell-8-output-1.png" width="758" height="566" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This visual effectively demonstrates the non-linear nature of the classification problem. The two classes are clearly divided by a curved, oscillating boundary rather than a straight line, highlighting the importance of using a flexible, proximity-based classifier like KNN. The figure confirms that the labels align closely with the sine-based threshold, validating the logic used to generate the labels and setting up an ideal test case for assessing KNN’s ability to capture non-linear patterns.</p>
</section>
<section id="generating-a-test-dataset" class="level3">
<h3 class="anchored" data-anchor-id="generating-a-test-dataset">Generating a Test Dataset</h3>
<p>To evaluate the generalization ability of the K-Nearest Neighbors (KNN) algorithm, we created a separate <strong>test dataset</strong> using the same data generation process as the training set, but with a different random seed to ensure new, unseen observations.</p>
<div id="45637924" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate test dataset with a different seed</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">99</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>n_test <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>x1_test <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n_test)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>x2_test <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n_test)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>boundary_test <span class="op">=</span> np.sin(<span class="dv">4</span> <span class="op">*</span> x1_test) <span class="op">+</span> x1_test</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> (x2_test <span class="op">&gt;</span> boundary_test).astype(<span class="bu">int</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>test_dat <span class="op">=</span> pd.DataFrame({<span class="st">'x1'</span>: x1_test, <span class="st">'x2'</span>: x2_test, <span class="st">'y'</span>: y_test})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="implementing-k-nearest-neighbors-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="implementing-k-nearest-neighbors-from-scratch">Implementing K-Nearest Neighbors from Scratch</h3>
<p>To classify new observations in the test dataset, we implemented the K-Nearest Neighbors (KNN) algorithm from scratch and evaluated its performance against Python’s built-in <code>KNeighborsClassifier</code> from the <code>scikit-learn</code> library. Both methods were configured with <span class="math inline">\(k=16\)</span> neighbors.</p>
<p>The custom implementation computes the Euclidean distance between each test point and all training points, selects the <span class="math inline">\(k\)</span> closest training instances, and assigns the most frequent label among them. In cases of ties, the algorithm defaults to the smallest label value. Predictions from this manual function were then compared with those produced by the <code>KNeighborsClassifier</code>, which was trained using the same training data and parameters.</p>
<p>To assess the performance, we measured classification accuracy and generated confusion matrices for both methods. Additionally, we computed the agreement rate — the proportion of test instances where both approaches predicted the same class. The results demonstrated strong alignment between the manual and <code>scikit-learn</code> implementations, validating the correctness of the custom algorithm and confirming its ability to replicate standard KNN behavior.</p>
<div id="a745802d" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial <span class="im">import</span> distance</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare training and test data</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> dat[[<span class="st">'x1'</span>, <span class="st">'x2'</span>]].values</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> dat[<span class="st">'y'</span>].values</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> test_dat[[<span class="st">'x1'</span>, <span class="st">'x2'</span>]].values</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> test_dat[<span class="st">'y'</span>].values</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Manual KNN function</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> knn_predict(X_train, y_train, X_test, k<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> []</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> X_test:</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute Euclidean distances to all training points</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        dists <span class="op">=</span> np.linalg.norm(X_train <span class="op">-</span> x, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Find the k nearest neighbors</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        k_indices <span class="op">=</span> np.argsort(dists)[:k]</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        k_labels <span class="op">=</span> y_train[k_indices]</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Count votes and break ties by choosing the smallest label</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>        label_counts <span class="op">=</span> Counter(k_labels)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        most_common <span class="op">=</span> <span class="bu">sorted</span>(label_counts.items(), key<span class="op">=</span><span class="kw">lambda</span> x: (<span class="op">-</span>x[<span class="dv">1</span>], x[<span class="dv">0</span>]))[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        predictions.append(most_common)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(predictions)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict using manual KNN</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>manual_preds <span class="op">=</span> knn_predict(X_train, y_train, X_test, k<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict using scikit-learn</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>sklearn_preds <span class="op">=</span> clf.predict(X_test)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare predictions</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>comparison <span class="op">=</span> pd.DataFrame({</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Manual_KNN'</span>: manual_preds,</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sklearn_KNN'</span>: sklearn_preds,</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Actual'</span>: y_test</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, confusion_matrix</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_knn_results(y_true, pred_manual, pred_sklearn):</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Manual k-NN Accuracy:"</span>, accuracy_score(y_true, pred_manual))</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Sklearn k-NN Accuracy:"</span>, accuracy_score(y_true, pred_sklearn))</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Agreement between the two methods</span></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    agreement <span class="op">=</span> np.mean(pred_manual <span class="op">==</span> pred_sklearn)</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Agreement between manual and sklearn:"</span>, agreement)</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Confusion matrices</span></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Confusion Matrix - Manual k-NN:"</span>)</span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(confusion_matrix(y_true, pred_manual))</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Confusion Matrix - Sklearn k-NN:"</span>)</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(confusion_matrix(y_true, pred_sklearn))</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>compare_knn_results(y_test, manual_preds, sklearn_preds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Manual k-NN Accuracy: 0.89
Sklearn k-NN Accuracy: 0.89
Agreement between manual and sklearn: 1.0

Confusion Matrix - Manual k-NN:
[[46  7]
 [ 4 43]]

Confusion Matrix - Sklearn k-NN:
[[46  7]
 [ 4 43]]</code></pre>
</div>
</div>
</section>
<section id="evaluating-knn-accuracy-across-varying-values-of-k" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-knn-accuracy-across-varying-values-of-k">Evaluating KNN Accuracy Across Varying Values of k</h3>
<p>To determine the optimal number of neighbors (<span class="math inline">\(k\)</span>) for the K-Nearest Neighbors algorithm, we systematically evaluated model performance across a range of <span class="math inline">\(k\)</span> values from 1 to 30. For each value of <span class="math inline">\(k\)</span>, we used our custom KNN implementation to classify the test dataset and recorded the percentage of correctly classified points. These results were plotted with <span class="math inline">\(k\)</span> on the horizontal axis and classification accuracy (in percentage) on the vertical axis.</p>
<p>The resulting plot reveals how predictive performance changes with increasing neighborhood size. Accuracy tends to fluctuate at lower values of <span class="math inline">\(k\)</span> due to overfitting, but stabilizes and peaks around a particular value. Based on the curve, the <strong>o</strong></p>
<div id="1a615bf1" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate accuracy for k from 1 to 30</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>ks <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">31</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>manual_accuracies <span class="op">=</span> []</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>sklearn_accuracies <span class="op">=</span> []</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> ks:</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    manual_preds_k <span class="op">=</span> knn_predict(X_train, y_train, X_test, k<span class="op">=</span>k)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    clf_k <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span>k)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    clf_k.fit(X_train, y_train)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    sklearn_preds_k <span class="op">=</span> clf_k.predict(X_test)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    acc_manual <span class="op">=</span> accuracy_score(y_test, manual_preds_k)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    acc_sklearn <span class="op">=</span> accuracy_score(y_test, sklearn_preds_k)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    manual_accuracies.append(acc_manual)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    sklearn_accuracies.append(acc_sklearn)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="9316d5a2" class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="hw4_questions_files/figure-html/cell-12-output-1.png" width="950" height="566" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>To determine the most effective number of neighbors for the K-Nearest Neighbors (KNN) algorithm, we evaluated classification accuracy across a range of values from <span class="math inline">\(k = 1\)</span> to <span class="math inline">\(k = 30\)</span>. For each <span class="math inline">\(k\)</span>, we recorded the percentage of correctly classified test points using both our manual KNN implementation and the built-in <code>scikit-learn</code> version. The results were plotted with <span class="math inline">\(k\)</span> on the horizontal axis and classification accuracy on the vertical axis.</p>
<p>From the accuracy curve, we observe that the performance peaks at <strong><span class="math inline">\(k = 1\)</span> and <span class="math inline">\(k = 2\)</span></strong>, achieving the highest accuracy of approximately <strong>92%</strong>. These values correspond to the model’s best predictive performance on the test set. As <span class="math inline">\(k\)</span> increases beyond 2, the accuracy gradually declines or fluctuates, indicating a trade-off between variance and bias. Lower values of <span class="math inline">\(k\)</span> offer greater model flexibility and can capture complex, localized decision boundaries—an advantage in this case, given the wiggly, non-linear nature of the data-generating process. However, larger <span class="math inline">\(k\)</span> values introduce more smoothing, which may result in misclassification near the boundary by incorporating distant or unrelated points into the majority vote.</p>
<p>Therefore, based on the plotted results, the <strong>optimal value of <span class="math inline">\(k\)</span> is either 1 or 2</strong>, as these settings achieve the highest test accuracy while effectively capturing the underlying structure of the dataset.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>